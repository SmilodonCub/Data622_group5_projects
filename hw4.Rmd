---
title: "Mental Health Survey"
subtitle: "Homework 4 Data 622 Section 2 Group 5"
author: "Bonnie Cooper, Leo Yi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: no
    theme: paper
    highlight: tango
    font-family: Consolas
    code_folding: hide
  pdf_document:
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

mark {
  background-color: whitesmoke;
  color: black;
}

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.width = 10)

options(scipen = 9)
```
<br>

## Introduction

We'll be working with a mental health dataset and will be conducting exploratory data analysis, unsupervised clustering, principal component analysis, gradient boosting, and support vector machines.

### Import Data

To begin, the following code will import the data and load the libraries:

```{r import}
library(stringr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(VIM)
library(corrplot)
library(purrr)
library(scales)
library(caret)
library(Hmisc)
library(naniar)
library(conflicted)
library(pheatmap)
library(corrplot)
library(factoextra)
library(gbm)
library(caret)
library(gridExtra)
library(MLmetrics)

# resolve function name conflict
conflict_prefer('filter', 'dplyr')
conflict_prefer('summarize', 'dplyr')

# import data
url <- 'https://raw.githubusercontent.com/SmilodonCub/Data622_group5_projects/main/ADHD_data.csv'
df <- read.csv(url, header=T, na.strings="")
```
<br>

### Variable Datatypes

In order to facilitate ease of use, we'll be renaming the columns. Additionally, we'll convert each of the number coded fields to factors while also including the proper labels.

```{r adjust column names}

# convert column names to lowercase
names(df) <- lapply(names(df), tolower)

# replace periods with underscore
names(df) <- str_replace_all(names(df), '\\.', '_')

# rename last column to remove trailing underscore
names(df)[ncol(df)] <- 'psych_meds'

# raw data with renamed columns
df_raw <- df

names(df_raw)

```

```{r factor datatype setup}

# Sex
df$sex <- factor(df$sex, levels = c(1,2), labels = c('Male','Female'))

# Race
df$race <- factor(df$race, levels = c(1,2,3,4,5,6), labels = c('White','African American','Hispanic','Asian','Native American','Other or Missing Data'))

# ADHD q1 - q18
adhd_cols <- names(df[,5:22])
df[adhd_cols] <- lapply(df[adhd_cols], factor, levels = c(0,1,2,3,4), labels = c('Never','Rarely','Sometimes','Often','Very Often')) 

# Mood Disorder q1a - q2
md_cols <- names(df[,24:37])
df[md_cols] <- lapply(df[md_cols], factor, levels = c(0,1), labels = c('No','Yes')) 

# Mood Disorder q3
df$md_q3 <- factor(df$md_q3, levels = c(0,1,2,3), labels = c('No Problem','Minor','Moderate','Serious')) 

# Substance Abuse
sa_cols <- names(df[,40:45])
df[sa_cols] <- lapply(df[sa_cols], factor, levels = c(0,1,2,3), labels = c('No Use','Use','Abuse','Dependence')) 

# Court Order
df$court_order <- factor(df$court_order, levels = c(0,1), labels = c('No','Yes'))

# Education
# think it might be okay to leave this as a number

# History of Violence, Disorderly Conduct, Suicide Attempt
hist_cols <- names(df[,48:50])
df[hist_cols] <- lapply(df[hist_cols], factor, levels = c(0,1), labels = c('No','Yes'))

# Abuse History
df$abuse <- factor(df$abuse, levels = c(0,1,2,3,4,5,6,7), 
                   labels = c('No','Physical','Sexual','Emotional','Physical & Sexual','Physical & Emotional','Sexual & Emotional','Physical, Sexual, & Emotional'))

# Non-Substance Related Drugs
df$non_subst_dx <- factor(df$non_subst_dx, levels = c(0,1,2), labels = c('None','One','More than one'))

# Substance Related Drugs
df$subst_dx <- factor(df$subst_dx, levels = c(0,1,2,3), labels = c('None','One','Two','Three or more'))

# Psychiatric Meds
df$psych_meds <- factor(df$psych_meds, levels = c(0,1,2), labels = c('None','One','More than one'))

# str(df)

```


## Exploratory Data Analysis

The following code will quantitatively and visually explore the nature of the dataset.  

We begin by describing the dataset features.

Use `dplyr`'s `glimpse()` function to take a quick look at the data structure. Followed by `Hmisc`'s `describe()` function to return some basic summary statistics about the dataframe features:

```{r}
# quick look at what the data structure looks like
glimpse(df)
```

From this output, we can summarize each dataset feature as follows:  

| **Column Numbers** |   **Column Labels**  |                                  **Description**                                  |
|:------------------:|:--------------------:|:---------------------------------------------------------------------------------:|
|          1         |       `initial`      | (string) subject's initials |
|          2         |         `age`        |                          (numeric) integer values (years)                         |
|          3         |         `sex`        |                    (categorical): binary ('male' and 'female')                    |
|          4         |        `race`        |                                   (categorical)                                   |
|        5-22        | `adhd_q1`-`adhd_q18` |                            (categorical) ordinal values                           |
|         23         |     `adhd_total`     |            (numeric): summary feature derived from `adhd_q1`-`adhd_q18`           |
|        24-38       |   `md_q1a`-`md_q3`   |                            (categorical) ordinal values                           |
|         39         |      `md_total`      |              (numeric): summary feature derived from `md_q1a`-`md_q3`             |
|         40         |       `alcohol`      |                           (categorical): ordinal values                           |
|         41         |         `thc`        |                           (categorical): ordinal values                           |
|         42         |       `cocaine`      |                           (categorical): ordinal values                           |
|         43         |     `stimulants`     |                           (categorical): ordinal values                           |
|         44         | `sedative_hypnotics` |                           (categorical): ordinal values                           |
|         45         |       `opioids`      |                           (categorical): ordinal values                           |
|         46         |     `court_order`    |                           (categorical): binary (yes/no)                          |
|         47         |      `education`     |                         (numeric): interger values (years)                        |
|         48         |   `hx_of_violence`   |                           (categorical): binary (yes/no)                          |
|         49         | `disorderly_conduct` |                           (categorical): binary (yes/no)                          |
|         50         |       `suicide`      |                           (categorical): binary (yes/no)                          |
|         51         |        `abuse`       |                           (categorical): ordinal values                           |
|         52         |    `non_subst_dx`    |                           (categorical): ordinal values                           |
|         53         |      `subst_dx`      |                           (categorical): ordinal values                           |
|         54         |     `psych_meds`     |                           (categorical): ordinal values                           |

The columns `adhd_q1`-`adhd_q18` and `md_q1a`-`md_q3` are summarized by the derivative columns `adhd_total` and `md_total` respectively. The **adhd** features correspond to an ADHD self-report survey whereas the **md** features give responses to a mood disorder self-report survey. For the initial Exploratory Data Analysis, the individual questions responses will be dropped in place of visualizations and summary statistics on the derived columns, `adhd_total` and `md_total`. A detailed analysis of the individual survey questions will be taken up in the Principal Components Analysis section.

Next, we visualize the extent of the missing values using the `naniar` library.


### Missing Values

Use `naniar`'s `miss_var_summary()` and `vis_miss()` functions to summarize and visualize the missing values in the features of the dataset:
```{r}
# return a summary table of the missing data in each column
miss_var_summary(df)
```


```{r}
# visualize the amount of missing data for each feature
vis_miss( df, cluster = TRUE )
```

The figure above shows a grouped view of the missing values in each feature column. Overall, 2.7% of the values are missing from the dataset. The majority of the features have no missing values. Many of the features have relatively few missing values. However, the `psych_meds` features is missing a considerable portion of the data.

```{r}
df <- df %>%
  select( -c(psych_meds) )
```


Having dropped `psych_meds`, we can now use the `gg_miss_upset()` function to look for any remaining patterns of missing values in the data.

```{r}
gg_miss_upset( df )
```

Both the `vis_miss()` and `gg_miss_upset()` visualization are informative, because they reveal some interesting patterns in the missing values for the data set. For instance, once we remove `psych_meds` most of the remaining rows that are not full, hold multiple missing values from a subset of the features. In other words, the remaining missing values are not randomly dispersed across the data set. Rather, they are concentrated in a subset of the features: `disorderly_conduct`, `suicide`, `abuse`, `non_subst_dx` and `subst_dx`. Notably, the responses that correspond to the ADHD and Mood Disorder self-reporting surveys are have no missing values; this knowledge will be helpful downstream. Further handling of missing values will be tailored to the analysis of each of the sections below.


### Exploratory Data Visualizations

To gain a feel for the data, we will evaluate visualizations of select features.

#### Distributions of Demographic Variables

Summary Statistics of Demographic Variables
```{r}
# summary of each demographic feature
demo_df <- df %>%
  select(c('age','sex','race','education'))#select( -c( 5:22, 24:38 ) )
describe( demo_df )
```


Visualizing the demographic information of the mental health study

```{r figDemo, fig.height = 5, fig.width = 12}
p1 <- ggplot( df, aes(x = age) ) +
  geom_density( fill="#69b3a2", color="#e9ecef", alpha=0.8 ) +
  theme_classic() +
  ggtitle( 'Age Density Plot' )
num_obs <- dim(df)[1]
p2 <- df %>%
  dplyr::count( sex ) %>%
  dplyr::mutate( n = n/num_obs ) %>%
  ggplot( aes(x=sex, y=n)) + 
  geom_bar(stat = "identity") +
  ylim( 0, 0.6 ) +
  ylab( 'proportion' ) +
  ggtitle( 'Distribution  of Sex (binary)' ) +
  theme_classic()
p3 <- df %>%
  dplyr::count( race ) %>%
  dplyr::mutate( n = n/num_obs ) %>%
  ggplot( aes(x=race, y=n)) + 
  geom_bar(stat = "identity") +
  ylab( 'proportion' ) +
  ylim( 0, 0.6 ) +
  ggtitle( 'Distribution  of Race' ) +
  theme_classic() + 
  scale_x_discrete(guide = guide_axis(n.dodge = 2))
p4 <- ggplot( df, aes(x = education) ) +
  geom_density( fill="#69b3a2", color="#e9ecef", alpha=0.8 ) +
  theme_classic() +
  ggtitle( 'Education Density Plot' )
num_obs <- dim(df)[1]
grid.arrange(p1, p4, p2, p3, ncol=2)
```
  
From these visualization we learn several things about the mental health study participants:  

1. **Age** - the age distribution is bimodal with one peak at approximate 26 and another at 46 and a range from 18 to 69 years of age.
2. **Education** - education has an interesting envelope. There is a sharp peak at ~12years (high school graduates) with long kurtosis to either side
3. **Sex** - sex is given as a binary feature and is not equally balanced with 57% Male and 43% Female
4. **Race** - race takes one of four values: 'White' (41%), 'African American' (57%), 'Hispanic'(.6%) and 'Other or Missing Data'(1%).


#### ADHD and Mood Disorder Survey Scores

Summary Statistics for self-report surveys
```{r}
# summary of each field
initial_eda_df <- df %>%
  select( c('adhd_total', 'md_total' ) )
describe( initial_eda_df )
```

Visualizing the `adhd_total` and `md_total` features. These two features give a summary score for the questions on an [ADHD Self-Reporting Survey](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf) and a separate [Mood Disorder Questionnaire](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC314375/). 

```{r figTotal, fig.height = 5, fig.width = 12}
p1 <- ggplot( df, aes(x = adhd_total) ) +
  geom_density( fill="#69b3a2", color="#e9ecef", alpha=0.8 ) +
  theme_classic() +
  ggtitle( 'ADHD Self-Reporting Survey Scores' )
p2 <- ggplot( df, aes(x = md_total) ) +
  geom_density( fill="#9D85FC", color="#e9ecef", alpha=0.8 ) +
  theme_classic() +
  ggtitle( 'Mood Disorder Self-Reporting Survey Scores' )
grid.arrange( p1, p2, ncol=2 )
```

`adhd_total` has a roughly normal distribution with a mean centered at 34.3 and a range of 0 to 72. `md_total` has an entirely different range from 0 to 17; it's distribution is more left-skewed than ADHD.  

Next we visualize `adhd_total` ~ `md_total` to evaluate the relationship between the two total scores
```{r}
ggplot(df, aes(x=md_total, y=adhd_total)) + 
  geom_point()+
  geom_smooth(method=lm) +
  theme_classic() +
  ggtitle('ADHD total ~ Mood Disorder total')
```

We can evaluate the fit of a linear model to the data:
```{r}
fit = lm( adhd_total ~ md_total, df )
summary( fit )
par(mfrow = c(2, 2))
p1 <- plot(fit, which=1)
p2 <- plot(fit, which=2)
p3 <- plot(fit, which=3)
p4 <- plot(fit, which=4)
par(mfrow = c(1, 1))
```

The fit is statistically significant and the diagnostics suggest that the assumptions for linearity are reasonably met. Therefore, we can say with confidence that for a given point increase in Mood Disorder Score, the average ADHD score increases 1.7769 points.  

For an EDA of the individual question responses for both the Mood Disorder and ADHD self-reporting surveys, please see the PCA section.

#### Substance Misuse and other Features

Summary Statistics for Substance Misuse Features
```{r}
# summary of each field
sub_df <- df %>%
  select( c( 'alcohol', 'thc', 'cocaine', 'stimulants', 'sedative_hypnotics', 'opioids', 'subst_dx' ) )
describe( sub_df )
```

The individual substance features (e.g. `opioids`) describe the severity with which a participants uses or does not use a particular substance. On the other hand, `subst_dx` counts how many substances a participant has a reported using irrespective of severity of use. The following figure will attempt to find patterns between the individual substances and the summary feature.

```{r}
none_count <- sum(sub_df$subst_dx == 'None')
one_count <- sum(sub_df$subst_dx == 'One')
two_count <- sum(sub_df$subst_dx == 'Two')
more_count <- sum(sub_df$subst_dx == 'Three or more')
total_count <- dim(sub_df)[1]
  
plot_df <- sub_df %>%
  drop_na() %>%
  #mutate_if(is.factor, as.numeric) %>%
  gather(var, value, -subst_dx) %>%
  group_by(var, value, subst_dx) %>%
  dplyr::summarize(count = n(),
            .groups = 'drop') %>%
  dplyr::mutate(prop = count / case_when(subst_dx == 'None' ~ none_count, 
                                         subst_dx == 'One' ~ one_count,
                                         subst_dx == 'Two' ~ two_count,
                                         subst_dx == 'Three or more' ~ more_count))
plot_df$value <- factor(plot_df$value,levels = c("No Use", "Use", "Dependence", "Abuse"))

ggplot(plot_df, aes(x = value, y = count, fill = subst_dx)) +
  geom_col(position="stack") +#, position = 'dodge') +
  facet_wrap(~var, scales = 'free') +
  theme_bw() +
  labs(y = 'count',
       x = element_blank(),
       title = 'Distributions For Substances') +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
  #scale_y_continuous(labels = percent_format(accuracy = 1))

```
From the figure above, we see that for each individual substance, the majority of participants report 'No use'. However, the instances of 'Dependence' are elevated for several substances notably alcohol, cocaine and THC. For these substances it is the case that there is a substantial proportion of participants that also reported a substance abuse diagnosis (`subst_dx`) of three or more substances. There is another interesting result in this figure: there are many instances where a level of substance use was self-reported for a participant that also reported 'None' for substance abuse diagnosis. It is interesting that a participant can simultaneously report to, for example, have a dependence on alcohol yet report 'None' for a substance abuse diagnosis. Perhaps more documentation on the dataset can shed light on this.

To re-represent this data, we will generate a new summary feature that sums the reported sevarity of use for each substance included in the self report:
```{r}
sub_df <- sub_df %>%
  mutate_if(is.factor, as.numeric) %>%
  dplyr::mutate( subst_sum = alcohol + thc + cocaine + stimulants + sedative_hypnotics + opioids )
```

We can derive a similar summary variable for the non-substance abuse behaviors reported (e.g. disorderly conduct, court order etc.). Hoever, we exclude `abuse` because it has a difference and non-binary reporting criteria. 

```{r}
abuse_df <- df %>%
  select( c( 'suicide', 'disorderly_conduct', 'hx_of_violence', 'court_order' ) ) %>%
  mutate_if(is.factor, as.numeric) %>%
  dplyr::mutate( abuse_sum = suicide + disorderly_conduct + hx_of_violence + court_order )
```


Visualize the distribution of the new summary variables
```{r figNewFeats, fig.height = 5, fig.width = 12}
p1 <- ggplot( sub_df, aes(x = subst_sum) ) +
  geom_density( fill="#69b3a2", color="#e9ecef", alpha=0.8 ) +
  theme_classic() +
  ggtitle( 'ADHD Self-Reporting Survey Scores' )
p2 <- ggplot( abuse_df, aes(x = abuse_sum) ) +
  geom_density( fill="#9D85FC", color="#e9ecef", alpha=0.8 ) +
  theme_classic() +
  ggtitle( 'Mood Disorder Self-Reporting Survey Scores' )
grid.arrange( p1, p2, ncol=2 )
```

For one last exploratory visualization of the data, we will look at correlations between the self-reporting survey summary features (`adhd_total` & `md_total`) and the new summary feature generated here. Obviously, a positive correlation is expected between the two survey totals, because we demonstrated the linear regression earlier.
```{r function, echo = F}

plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}

```

```{r corrplot, fig.height = 8}
df_feats <- df %>%
  select( c('adhd_total','md_total') ) %>%
  dplyr::mutate( subst_sum = as.integer( sub_df$subst_sum ),
          abuse_sum = as.integer( abuse_df$abuse_sum ) ) %>%
  drop_na()

corrplot(cor(df_feats), method="number")
```
From the correlation plot above, we do not see a strong relationship between the self-reported survey total scores and the new features we generated to summarize substance abuse and non-substance abuses. However, in upcoming sections we will use machine learning techniques to investigate this relationship further.



## Clustering Methods


### Finding K

In order to find patients of clusters, we first need to determine the number of clusters. Below, we'll use two common methods, evaluating the sum of squares and silhouette width, in order to decide how many clusters we'll use. Since clustering requires no null values, we'll use the bagImpute method from caret and use the raw data before each field was converted to factors.

```{r cluster data prep}

# exclude names
df3 <- df_raw %>%
  select(-initial)

# impute null
preproc <- preProcess(df3, 'bagImpute')
df4 <- predict(preproc, df3)

```

#### Total Within Sum of Square

```{r cluster wss}

fviz_nbclust(df4, kmeans, method = "wss")

```

Based on the sum of squares for each value of K, we can use 2 clusters as the 'elbow' point. This gives us a good starting point to determine K, but we should note that in unsupervised learning there is no definite value of k. For the purposes of this homework assignment, we'll choose one value of k and proceed with the analysis, but in real life situations, it could be helpful to try multiple values of k and analyze the results to find anything that might be interesting.

#### Average Siluotte Width

```{r cluster silhouette}

fviz_nbclust(df4, kmeans, method = "silhouette")

```

Based on the average silhouette width of clusters, the optimal number of clusters shows k = 2.

Since both methods above have a consensus on two clusters, we'll proceed with calculating the clusters using k = 2 and then plot the distributions to observe and note any relative differences.

### Calculating Clusters

Once the clusters are calculated, let's check the size of each cluster to make sure we're looking at different groups with enough data points.

```{r cluster calc}

# 25 random starts
cluster_k2 <- kmeans(df4, 2, nstart = 25)

# add cluster number to dataframe
df4$cluster_k2 <- cluster_k2$cluster

# check cluster sizes
table(df4$cluster_k2)

```

Using k = 2, we created an almost even split of observations. Cluster 1 has 88 observations and cluster 2 has 87.

Note that when this file is run, the cluster numbers may switch or possibly form different clusters altogther. We used the paramter nstart = 25, which increases the chance that we'll end up with the same cluster. It's possible that the algorithm ends up with a very close vote, like 12 to 13 or 11 to 14 -- which could lead to different clusters and not just switched cluster labels.

#### ADHD Total vs Suicide Plot

```{r cluster viz adhd}

# adhd vs suicide
fviz_cluster(cluster_k2, data = df4, c('adhd_total', 'suicide'))

```

The clusters created show a clear delineation for respondents based on `adhd_total`. This suggests that the clusters are separated based on levels of ADHD. Note that the variables have not been scaled. Since the ADHD total has a wider variance than each of the individual adhd question variables, it's likely that different clusters would be formed if all the data was scaled. This could really affect the forming of the clusters since the variance of the total variable is much larger than a single question, which may range from 0-4.

#### Mood Disorder Total vs Suicide Plot

```{r cluster viz mood disorder}

# mood disorder vs suicide
fviz_cluster(cluster_k2, data = df4, c('md_total', 'suicide'))

```

The results of the k2 cluster show considerable overlap based on `md_total`. This means that the clusters aren't separated by mood disorder levels. Note that the mood disorder total variable is not scaled, similar to the ADHD total variable. Because the clusters are overlapped, that tells us that the cluster weren't formed simply because of the wide variance, but rather because these two clusters have differences in other ways.

### Relative Cluster Differences

Let's examine relative distributions for each cluster and variable. We decided to use probability density charts because we're most interested in relative differences in proportion of the clusters.

```{r cluster plot function}

# function to plot relative variable distributions for clusters
cluster_plot <- function(start_col, end_col, plot = 'density', legend_position = 'bottom') {

  temp_df <- df4[,start_col:end_col] %>%
    bind_cols(cluster = factor(df4$cluster_k2)) %>%
    gather(var, val, -cluster)

  if (plot == 'density') {
    p <- ggplot(temp_df, aes(x = val, fill = cluster)) +
    geom_density(alpha = 0.3)
  } else if (plot == 'histogram') {
    p <- ggplot(temp_df, aes(x = val, fill = cluster)) +
    geom_histogram(alpha = 0.3)
  }
  
  p +
    facet_wrap(~var, scales = 'free') +
    theme_bw() +
    labs(fill = 'Cluster',
         x = element_blank(),
         y = element_blank()
         ) +
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          axis.text.x = element_text(size = 8),
          legend.position = legend_position)

}

```

#### Demographics

```{r, fig.height=3}

cluster_plot(1,3)

```

- Cluster 1 shows a bimodal distribution of age, with modes near 25 and 45. Cluster 2 has fewer younger people with a higher single mode, near 45 years of age.
- Cluster 2 is more likely to be African American.
- Cluster 1 is more likely to be female and Cluster 2 is more likely to be male.

#### ADHD 

```{r cluster adhd, fig.height = 7}

cluster_plot(4,22)

```

With clear differences for each question as well as the total, cluster 1 reported higher adhd values relative to cluster 2.

#### Mood Disorders

```{r cluster mood disorders, fig.height = 7}

cluster_plot(23,38)

```

Some questions have similiar distributions, but generally overall as well as altogether, cluster 1 is more likely to report higher levels of mood disorders.

#### Substance Abuse

```{r cluster substance abuse}

cluster_plot(39,44)

```

There are different drug use patterns for both of the classes depending on the type of drug.

- Alcohol: Both clusters show similar usage patterns for alcohol, with cluster 1 having a slightly higher chance of using, abusing, and being dependent. Cluster 2 has a higher chance of not drinking.
- Cocaine: Cluster 1 is twice as likely as cluster 2 to not use cocaine and cluster 2 has a relatively slightly higher chance of being dependent.
- Opioids: Cluster 2 has a higher chance of not using.
- Sedative Hypnotics: Cluster 2 has a slighly higher chance of not using.
- Stimulants: Cluster 2 is approximately twice as likely to not use stimulants.
- THC: Cluster 1 more likely to both not use and also to be dependent.

#### Substance Abuse Levels

```{r cluster substance abuse levels, fig.height=3}

cluster_plot(51,53)

```

Cluster 2 has a higher chance of not taking non-substance abuse related drugs and is more likely to take one substance related drug.

#### Social

```{r}

cluster_plot(45,50)

```

Cluster 2 is less likely to experience abuse and suicide, and more likely to have a history of violence and be charged with disorderly conduct.


## Principal Component Analysis

In this section we will explore the ADHD data set further using Principal Component Analysis (PCA). PCA is a dimensionality reduction method that can be very useful for detecting patterns in complex, feature-rich data. At it's core, PCA is a fairly simple linear algebra manipulation, an eigendecomposition. Basically, PCA maps the data onto a new set of components that consist of orthogonal axes (eigenvectors) and magnitudes (eigenvalues). Each successive component is added such that it describes as much remaining variance in the data as possible. From the resulting components we can select a subset that describe most of the variance in a new, lower-dimensional feature space which is more convenient for observing relationships in the data that were previously masked by the data's complexity.  

To approach this analysis, we derive 2 sets of features from the original data. Data features that correspond to 1) self-reported answers for an ADHD survey, 2) self-reported answers for a Mood Disorder survey.

### PCA: ADHD Self-Report Survey

[The Adult ADHD Self-Reporting Scale Symptoms Checklist](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf) is a survey consisting of 18 questions that all range on an ordinal categorical scale: 'Never', 'Rarely', 'Sometimes', 'Often', 'Very Often'

#### Preprocessing

To prepare the ADHD survey responses we performed the following:  

* selected the ADHD survery question responses and exclude the feature `adhd_total` which gives a summary score for the survey
* PCA works best with numeric data, so we will transform the ordinal categorical features to numeric values
* evaluate the presence of missing values in the data

```{r}
# preparing the ADHD self-report questionnaire data
adhd_df <- df %>%
  select( starts_with("adhd_") ) %>%
  select( -'adhd_total' )
# recode all questions from factor to numeric values
adhd_df <- adhd_df %>%
  mutate_at( 
    vars(matches("adhd_q") ), 
         funs( recode_factor( ., 'Never'=0,'Rarely'=1,'Sometimes'=2,'Often'=3,'Very Often'=4 ) ) )  %>%
  mutate_if( is.factor, as.numeric )
paste( 'initial number of rows:', dim( adhd_df )[1] )
```

Summary of missing responses in the ADHD Self-Reporting Survey:
```{r}
# return a summary table of the missing data in each column
miss_var_summary( adhd_df )
```

We see that there is only 1 missing value, so it seems reasonable to just drop it
```{r}
# remove missing
adhd_dna <- adhd_df %>%
  drop_na()
paste( 'after drop_na number of rows:', dim( adhd_dna )[1] )
```

#### PCA from scratch

We can perform PCA easily from scratch and use the intermediate steps to learn more about the relationship of the individual question features to the ADHD total score:  

* First: we need to scale the data. 
  - This we de-mean the data and normalize the range of all features. Perhaps, redundant because all questions for this survey are on the same scale.
  - use the `scale()` function
* Second: we find the covariance matrix
  - use the `cov()` function

```{r}
# scale the data to facilitate PCA
adhd_df_scaled <- scale( adhd_dna )
# create a covariance matrix
adhd_df_cov <- cov( adhd_df_scaled )
```

Visualizing the covariance matrix
```{r}
pheatmap(adhd_df_cov, display_numbers = T, color = colorRampPalette(c('white','red'))(100), cluster_rows = F, cluster_cols = F, fontsize_number = 15)
```

The values off the diagonal of the matrix describe the degree of covariance between the two features. We see that some pairs of questions are more closely related than others. For instance, questions #5 and #13 have the highest covariance:  

* `adhd_q5` - "How often do you fidget or squirm with your hands or feet when you have to sit down for a long time?"
* `adhd_q13` - "How often do you feel restless or fidgety"

Perhaps it is no surprise that subjects who report the frequency they fidget when sitting down for a long time report a similar incidence of general fidgeting/squirming.  

Another closely covarying pair of questions are #16 and #18  

* `adhd_q16` - "When you’re in a conversation, how often do you find yourself finishing the sentences of the people you are talking to, before they can finish them themselves?"
* `adhd_q18` - "How often do you interrupt others when they are busy?"

Again, these are two very similar questions.  
It is equally interesting to evaluate the questions with the lowest covariance, questions #3 and #16(see above).  

* `adhd_q3` - "How often do you have problems remembering appointments or obligations?"

Indeed, conversation skills and the ability to remember appointments are two (subjectively) different tasks, so it is reasonable to not see a strong relation between the two.  

Moving on, the covariance matrix can now be used for an eigendecomposition to find the principal components that describe the most variance in the data
```{r}
# eigendecomposition of the covariance matrix
eig <- eigen( adhd_df_cov )
explained_variance <- eig$values
components <- eig$vectors
explained_variance
```

Now, by storing the eigen values in a dataframe, we can explore the amount of variance explained by the PCA components. Furthermore, we find the ratio of explained variance by normalizing the cumulative variance of the components
```{r}
compnum <- 1:length( explained_variance )
exv_cumsum <- cumsum( explained_variance )/length( explained_variance )
pca_res <- data.frame( compnum, explained_variance, exv_cumsum )
pca_res
```

Visualizing the cumulative explained variance described by the principal components with a Skree Plot:
```{r}
ggplot( pca_res, aes( x = compnum, y = exv_cumsum ) ) +
  geom_line() +
  ylim( c(0,1.1) ) +
  scale_x_continuous(breaks = seq(0, length( explained_variance ), by = 1)) +
  geom_hline( yintercept = 0.95, linetype = 'dotted', col = 'red') +
  annotate("text", x = 2, y = 0.98, 
           label = expression( "95%" ~ sigma), vjust = -0.5) +
  theme_minimal() +
  xlab( 'Principal Component Number' ) +
  ylab( 'Cummulative Explained Variance' ) +
  ggtitle( 'Scree Plot: Explained Variance of Principal Components' )
```

The figure above plots the cumulative explained variance of the ADHD survey questions and total score as a result of the eigendecomposition. We see that the first principal component accounts for `r paste0( round( exv_cumsum[1]*100,1 ), '%' )` of the total variance. Adding successive components gradually increases the cumulative explained variance. It takes as many as 14 components to describe 95% of the data's variance. Therefore, dimensionality reduction by way of PCA only moderately decreases the components necessary to adequately explain the data (with a threshold of 95%).

#### PCA using `prcomp()`

We can determine the principal components of the ADHD data more quickly using the `prcomp()` function from the `stats` library.
```{r}
# use the prcomp function
adhd_dna.pca <- prcomp( adhd_dna, center = TRUE, scale = TRUE )
# print the pca summary
summary( adhd_dna.pca )
```

It is interesting to compare the Cumulative Proportion from the `prcomp()` output with the feature `pca_res$exv_cumsum`; the results are the same for both the built-in PCA function as well as are PCA from scratch (a relief!).

The PCA object that results from `prcomp()` can be used to visualize the components using the `ggbiplot` library:  
```{r}
library( ggbiplot )
ggbiplot( adhd_dna.pca )
```

The biplot shown above renders the data in the feature space of principal components PC1 and PC2; these are the components that explain most of the variance in the data. The scatterplot points show the projection of the data onto PC1 & PC2. The red arrows represent the original feature space that the data was represented in. We can see that the original features have the largest projection onto the PC1 axis. Relatively few original features have a sizeable projection onto PC2 (e.g. `adhd_q16` & `adhd_q17`). Additionally, these vectors all have a positive component along PC1 showing that the result for each question has a positive relation to PC1. The size of the angle between a given pair of vectors is inversely related to the correlation of the pair. For example, we see that `adhd_q3` and `adhd_q16` have the largest angle between them; this corresponds to our earlier observation looking at the covariance matrix. 

### Mood Disorders self-report

The [Mood Disorder Questionnaire](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC314375/) is a self-reporting survey that can be used to identify subjects that may be bipolar.  

We can perform a similar PCA analysis for the Mood Disorder survey results. Taking similar preprocessing steps: 1) select mood disorder question features, evaluate missing values... 
```{r}
# prepare the mood disorder data and recode all questions from factor to numeric values
md_df <- df %>%
  select( starts_with("md_") ) %>%
  select( -'md_total' ) %>%
  mutate_if( is.factor, as.numeric )

# return a summary table of the missing data in each column
miss_var_summary( md_df )
```

Wonderful, there are no missing values, so we can proceed with PCA using `prcomp()`...
```{r}
# use the prcomp function
md_df.pca <- prcomp( md_df, center = TRUE, scale = TRUE )
# print the pca summary
summary( md_df.pca )
```

Now to visualize the PCA biplot:
```{r}
ggbiplot( md_df.pca ) + xlim( -2,2 )
```

A similar pattern emerges for the Mood Disorder survey responses. We see that all the projections again have a major component along PC1. Again, we can infer relationships between features from the biplot: `md_q1c` and `md_q3` have the widest angle separating them and are therefore the least correlated features. where:  

* `md_q1c` - Has there been a period of time when (...) you felt much more self-confident than usual
  - encoding - No:0, Yes:1
* `md_q3` -  How much of a problem did any of these cause you - being unable to work; having family, money or legal troubles; getting into arguments or fights
  - encoding - No Problem:0, Minor Problem:1, Moderate Problem:2, Serious Problem:3
  
Responses to `md_q1c` with a 'Yes' reflect a positive affect with a larger numeric value. Conversely, larger numeric values for `md_q3` are associated with a negative aspect. In light of the opposite score polarities, it is not surprising to see a lack of correlation between these two features in the PC1/PC2 space.   

On major difference between the Mood Disorder and ADHD biplot is that all the feature vectors point in a negative direction in relation to PC1. This suggests inverse relationship between the features variables and PC1. You may recall from the previous section that the responses to the ADHD survey were all positive in relation to PC1.

## Gradient Boosting

Gradient Boosting builds a prediction model from an ensemble of smaller 'weak learner' models. In this section we will implement the gradient boosting approach to fit a binary classification model to features from the ADHD data set to predict whether a patient attempts suicide.

### Data Preparation

Before fitting a Gradient Boost Model, we will collect a new subset of features for this approach. Instead of including all features for individual questions on either of the summaries, we will only be using the total scores (`adhd_total` and `md_total`) because the total scores are composite features build from the scores on the questions.
```{r}
ordinal_cats <- c( 'sex', 'race', 'abuse', 'alcohol', 'thc', 'cocaine', 'stimulants', 'sedative_hypnotics', 'opioids', 'suicide',
                   'court_order', 'hx_of_violence', 'disorderly_conduct', 'non_subst_dx', 'subst_dx')
gboost_df <- df %>%
  select( -starts_with("md_q") ) %>%
  select( -starts_with("adhd_q")) %>%
  select( -c('initial') ) %>%
  dplyr::mutate( across( all_of( ordinal_cats ), as.numeric ) ) %>%
  dplyr::mutate( suicide = suicide-1 )
paste( 'number of rows:', dim( gboost_df )[1] )
```
Evaluate the presence of missing data
```{r}
miss_var_summary( gboost_df )
```

```{r}
gg_miss_upset( gboost_df )
```

We see that there are approximately two dozen rows that are missing values for multiple features. Therefore, instead of imputing the missing, we will simply drop these rows.

```{r}
gboost_df <- gboost_df %>%
  drop_na() #%>%
  #mutate_if(is.factor, as.numeric)
paste( 'number of rows:', dim( gboost_df )[1] )
miss_var_summary( gboost_df )
```


We will begin by splitting the data into train and test sets

```{r}
set.seed(101)
indexes = createDataPartition(gboost_df$suicide, p = .75, list = F)
train = gboost_df[indexes, ]
test = gboost_df[-indexes, ]
```

Now, we use the train split to train a generalized boosted model using the `gbm()` function for a bernoulli distribution because our target, `suicides` has only two values.
```{r}
# Train model with preprocessing & repeated cv
gbm.model = gbm(suicide~., 
                data=train, 
                shrinkage=0.01, 
                distribution = 'bernoulli', 
                cv.folds=5, 
                n.trees=3000, 
                verbose=F)
gbm.model
```
```{r}
summary( gbm.model )
```

From this result, we take the iteration with the best performance:
```{r}
best.iter = gbm.perf(gbm.model, method = 'cv' )
```
```{r}
best.iter
```

The tree structure determined from the best performing iteration is then used to train a new model with these settings using the `caret` library.
```{r}
set.seed(123)
fitControl = trainControl(method="cv", number=5, returnResamp = "all")

train$suicide <- factor( train$suicide )

model_bestiter = train(suicide~., 
                       data=train, 
                       method="gbm",
                       distribution="bernoulli", 
                       trControl=fitControl,
                       verbose=F, 
                       tuneGrid=data.frame(.n.trees=best.iter, 
                                           .shrinkage=0.01, 
                                           .interaction.depth=1, 
                                           .n.minobsinnode=1))
```


Model Summary: 
```{r}
model_bestiter
```

One way to evaluate the performance of the model is by looking at the confusion matrix. This gives us an idea of how well the model performs at correctly classifying the two possible outcomes.
```{r}
confusionMatrix( model_bestiter )
```

Another way of evaluating the performance of our model is with a lift chart:
```{r}
test$suicide <- factor( test$suicide )
testres = predict( model_bestiter, test, type = 'prob')
testres$obs = test$suicide
testres$pred = predict(  model_bestiter, test )
multiClassSummary(testres, lev = levels(testres$obs))
```
```{r}
test$suicide <- factor( test$suicide )
testres = predict( model_bestiter, test, type = 'prob')
testres$obs = test$suicide
testres$pred = predict(  model_bestiter, test )
multiClassSummary(testres, lev = levels(testres$obs))
```
```{r}
evalResults <- data.frame(Class = test$suicide)
evalResults$GBM <- predict( model_bestiter, test, type = "prob")[,'0']
trellis.par.set(caretTheme())
liftData <- caret::lift(Class ~ GBM, data = evalResults)
plot(liftData, values = 60, auto.key = list(columns = 1,
                                            lines = TRUE,
                                            points = FALSE))
```

From this lift chart, we can see that to find 60% of the hits, ~55% of the data. The shaded region bounds ranges of performance for models of the data: the diagonal (longest side of the triangle) represents chance performance whereas the other two sides of the shaded triangle bound a classifier that finds all the targets for a given sample size (perfect classifier). We can see that our Gradient Boost Model performs better, but not much better than chance.  
Perhaps an SVM classifier can perform better....



## Support Vector Machine

In this section we'll start training a model using the linear kernel, with all the available variables and use that as a benchmark for any other SVM models we may train. The idea is to minimize test error and maximize interpretability. In the real world, being able to interpret the model will help one explain what makes the model work, but in some cases understanding how the model works takes a back seat to pure predictive power. This model won't be used to make any life altering predictions, so we'll discuss the results along the way.

We'll reuse the same dataframe used in the earlier clustering section to create training and test sets in order to preserve as many rows as possible. As a quick reminder, the raw data had the `psych_meds` field omitted, and the rest of the missing values imputed using the `bagImpute` method. The only adjustment we will make will be to exclude any observations without suicide data in order to avoid any confounding assumptions. We'll also be setting the tune length to 8, which is an arbitrary choice. We would like to set the number high enough to tune through a number of variations, but not too high where it would take too much processing power and time to train one model.

All models below will be trained using 10 fold cross validation and will have all variables centered and scaled before training. Using a 75/25 split leaves us with 122 training and 40 test observations. Note that the bag impute method fills in decimals where the variable may refer to a question with discrete answers. This may not be ideal, but is acceptable because each of the questions follow a similar scale of none/low to more/high in increasing order.

Finally, the last thing to note is that the dataset we're working with has about 30% of observations flagged for suicide.

```{r train test split df4}

df5 <- df3 %>%
  filter(!is.na(suicide)) %>%
  select(-psych_meds)

bag_imp <- preProcess(df5, 'bagImpute')
df6 <- predict(bag_imp, df5)

df6$suicide <- factor(df6$suicide)

set.seed(101)
trainIndex <- createDataPartition(df6$suicide,
                                  p = 0.75,
                                  list = F)

train <- df6[trainIndex,]
test <- df6[-trainIndex,]

ctrl <- trainControl(method = 'cv', number = 10)
tune_length <- 8

```

### All Variables

#### SVM Linear, All Variables

```{r svm linear all vars}

svm_linear <- train(suicide ~ .,
                    data = train,
                    method = 'svmLinear',
                    preProc = c('center', 'scale'),
                    tuneLength = tune_length,
                    trControl = ctrl
                    )

test$svm_linear <- predict(svm_linear, test)

svm_linear

```

Here are the results from the first SVM linear model show about 66% accuracy on the training data, which is not a great start. For a classification model with 30% suicides, we can reach 70% accuracy by guessing that no one commits suicide. The model doesn't seem to be predicting all negatives, but still isn't very accurate. We'll calculate the predictions on the test data and compare the test error after all the SVM models are built.

#### SVM Poly, All Variables

Next, let's train another SVM model with the same parameters as the linear model, only changing the method to use a polynomial to see if we can reach higher levels of accuracy. By keeping all the conditions the same except for the method, we can see which method fits better and is a more appropriate choice.

```{r svm poly all vars}

svm_poly <- train(suicide ~ .,
                  data = train,
                  method = 'svmPoly',
                  preProc = c('center', 'scale'),
                  tuneLength = tune_length,
                  trControl = ctrl
                  )

test$svm_poly <- predict(svm_poly, test)

x <- svm_poly$bestTune %>%
  row.names()

svm_poly$results[x,]

```

Looking at the results of the best model above, the tuned polynomial method produced a higher level of accuracy, around 75%. Although the results were slightly better than the linear model, we can't make the case that this model is better before looking at the relative test errors.

#### SVM Radial, All Variables

The next method we'll try is the radial method. This will allow for a circular boundary rather than a straight or curved line.


```{r svm radial all vars}

svm_radial <- train(suicide ~ .,
                    data = train,
                    method = 'svmRadial',
                    preProc = c('center', 'scale'),
                    tuneLength = tune_length,
                    trControl = ctrl
                    )

test$svm_radial <- predict(svm_radial, test)

x <- svm_radial$bestTune %>%
  row.names()

svm_radial$results[x,]

```

Looking at the results above, the radial method produced a comparable model with about 72% accuracy. This model performed better than the linear model and worse than the polynomial method, with all 3 models within similar range. 

If one of the two methods performed significantly better than the other, we could move forward and stick with that method. The next step we'll take is to reconsider the variables included and then run the models again. If we were short on time and resources, it would make sense to continue with just the best performing polynomial method, but since we're experimenting and exploring here, we'll run all three methods again and see what we get.

Also note while writing up this report, each model was run multiple times and the results were not consistent for each model. It's possible that the accuracy levels and results shown may vary when producing the final report.

### Limited Variables

When building models, collinearity and noise can present hurdles to creating a production quality model. Maybe one day we'll have computers that are fast enough to train every single combination of variables and pick the best one, but for now we're going to consider all of the variables and decide which ones to move forward with.

It would make sense for us to try excluding all of the individual ADHD and mood disorder questions, since the totals for each respective state of mind allows us to get a higher level view of these variables. Below, we'll create a copy of the training dataset excluding the individual questions, the retrain the three types of SVM models and evaluate the training results.

Finally, we'll compare the test results for each of the 6 models.

#### Training DataFrame v2

```{r train v2}

train2 <- train %>%
  select(age, sex, race, adhd_total, md_total) %>%
  bind_cols(train[,39:52])

```

#### SVM Linear v2

```{r svm linear2}

svm_linear2 <- train(suicide ~ .,
                     data = train2,
                     method = 'svmLinear',
                     preProc = c('center', 'scale'),
                     tuneLength = tune_length,
                     trControl = ctrl
                     )

test$svm_linear2 <- predict(svm_linear2, test)

svm_linear2

```

Looking at the results of the 2nd linear SVM model, the accuracy is about the same as the first, but it doesn't seem like much of an improvement. Based on these results alone, it doesn't look like we've gained any predictive power, but we have improved the overall interpretability.

#### SVM Polynomial v2

```{r svm poly2}

svm_poly2 <- train(suicide ~ .,
                   data = train2,
                   method = 'svmPoly',
                   preProc = c('center', 'scale'),
                   tuneLength = tune_length,
                   trControl = ctrl
                   )

test$svm_poly2 <- predict(svm_poly2, test)

x <- svm_poly2$bestTune %>%
  row.names()

svm_poly2$results[x,]

```

The 2nd polynomial SVM model has the highest accuracy so far of all models with a slight lead. This model is comparable to the 1st polynomial SVM, giving additional evidence that the omission of the individual ADHD and mood disorder questions is mainly adding value by creating relatively easier to interpret models.

#### SVM Radial v2

```{r svm radial2}

svm_radial2 <- train(suicide ~ .,
                     data = train2,
                     method = 'svmRadial',
                     preProc = c('center', 'scale'),
                     tuneLength = tune_length,
                     trControl = ctrl
                     )

test$svm_radial2 <- predict(svm_radial2, test)

x <- svm_radial2$bestTune %>%
  row.names()

svm_radial$results[x,]

```

The second radial model using fewer variables has seen a considerable drop in accuracy. We will need to check the test error of the models to be certain that the accuracy of the model has truly dropped.

### SVM Test Errors

```{r svm test results}

data.frame(rbind(
  postResample(test$svm_linear, test$suicide),
  postResample(test$svm_poly, test$suicide),
  postResample(test$svm_radial, test$suicide),
  postResample(test$svm_linear2, test$suicide),
  postResample(test$svm_poly2, test$suicide),
  postResample(test$svm_radial2, test$suicide)
),
row.names = c('SVM linear, all variables', 
              'SVM poly, all variables',
              'SVM radial, all variables', 
              'SVM linear, limited', 
              'SVM poly, limited',
              'SVM radial, limited'
              )
)

```

When looking at the accuracy of all 6 models on the test data, we can tell that no single model stands out. With all models reaching approximately 70% accuracy, we're about the same level of accuracy compared to always predicting that no one commits suicide.


```{r}

library(GGally)

ggpairs(train2,
        mapping=ggplot2::aes(colour = suicide, alpha = 0.3))

```

Next, we tried a pairs plot to look for any obvious separations. The idea is to check for any possible two variable combinations where we could clearly separate people who had attempted suicide or not. This method is limited to two dimensions as it's difficult to visualize higher dimensions.


## References

1. [ADHD symptoms and suicide attempts in adults with mood disorders: An observational naturalistic study](https://www.sciencedirect.com/science/article/pii/S2666915321001505)
2. [The Adult ADHD Self-Reporting Scale Symptoms Checklist](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf)
3. [Mood Disorder Questionnaire](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC314375/)