---
title: "Mental Health Survey"
subtitle: "Homework 4 Data 622 Section 2 Group 5"
author: "Bonnie Cooper, Orli Khaimova, Leo Yi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: no
    theme: paper
    highlight: tango
    font-family: Consolas
    code_folding: hide
  pdf_document:
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

mark {
  background-color: whitesmoke;
  color: black;
}

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.width = 10)

options(scipen = 9)

```
<br>

## Introduction

We'll be working with a mental health dataset and will be conducting exploratory data analysis, unsupervised clustering, principal component analysis, gradient boosting, and support vector machines.

### Import Data

To begin, the following code will import the data and load the libraries:

```{r import}
library(stringr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(VIM)
library(corrplot)
library(purrr)
library(scales)
library(caret)
library(Hmisc)
library(naniar)
library(conflicted)
library(pheatmap)
library(corrplot)
library(factoextra)
library(gbm)
library(caret) 

# resolve function name conflict
conflict_prefer('filter', 'dplyr')
conflict_prefer('summarize', 'dplyr')

# import data
url <- 'https://raw.githubusercontent.com/SmilodonCub/Data622_group5_projects/main/ADHD_data.csv'
df <- read.csv(url, header=T, na.strings="")

```
<br>


### Variable Datatypes

In order to facilitate ease of use, we'll be renaming the columns. Additionally, we'll convert each of the number coded fields to factors while also including the proper labels.

```{r adjust column names}

# convert column names to lowercase
names(df) <- lapply(names(df), tolower)

# replace periods with underscore
names(df) <- str_replace_all(names(df), '\\.', '_')

# rename last column to remove trailing underscore
names(df)[ncol(df)] <- 'psych_meds'

# raw data with renamed columns
df_raw <- df

names(df)

```

```{r factor datatype setup}

# Sex
df$sex <- factor(df$sex, levels = c(1,2), labels = c('Male','Female'))

# Race
df$race <- factor(df$race, levels = c(1,2,3,4,5,6), labels = c('White','African American','Hispanic','Asian','Native American','Other or Missing Data'))

# ADHD q1 - q18
adhd_cols <- names(df[,5:22])
df[adhd_cols] <- lapply(df[adhd_cols], factor, levels = c(0,1,2,3,4), labels = c('Never','Rarely','Sometimes','Often','Very Often')) 

# Mood Disorder q1a - q2
md_cols <- names(df[,24:37])
df[md_cols] <- lapply(df[md_cols], factor, levels = c(0,1), labels = c('No','Yes')) 

# Mood Disorder q3
df$md_q3 <- factor(df$md_q3, levels = c(0,1,2,3), labels = c('No Problem','Minor','Moderate','Serious')) 

# Substance Abuse
sa_cols <- names(df[,40:45])
df[sa_cols] <- lapply(df[sa_cols], factor, levels = c(0,1,2,3), labels = c('No Use','Use','Abuse','Dependence')) 

# Court Order
df$court_order <- factor(df$court_order, levels = c(0,1), labels = c('No','Yes'))

# Education
# think it might be okay to leave this as a number

# History of Violence, Disorderly Conduct, Suicide Attempt
hist_cols <- names(df[,48:50])
df[hist_cols] <- lapply(df[hist_cols], factor, levels = c(0,1), labels = c('No','Yes'))

# Abuse History
df$abuse <- factor(df$abuse, levels = c(0,1,2,3,4,5,6,7), 
                   labels = c('No','Physical','Sexual','Emotional','Physical & Sexual','Physical & Emotional','Sexual & Emotional','Physical, Sexual, & Emotional'))

# Non-Substance Related Drugs
df$non_subst_dx <- factor(df$non_subst_dx, levels = c(0,1,2), labels = c('None','One','More than one'))

# Substance Related Drugs
df$subst_dx <- factor(df$subst_dx, levels = c(0,1,2,3), labels = c('None','One','Two','Three or more'))

# Psychiatric Meds
df$psych_meds <- factor(df$psych_meds, levels = c(0,1,2), labels = c('None','One','More than one'))

# str(df)

```


## Exploratory Data Analysis

The following code will quantitatively and visually explore the nature of the dataset.  

We begin by describing the dataset features.

Use `dplyr`'s `glimpse()` function to take a quick look at the data structure. Followed by `Hmisc`'s `describe()` function to return some basic summary statistics about the dataframe features:

```{r}
# quick look at what the data structure looks like
glimpse(df)
```

From this output, we can summarize each dataset feature as follows:  

| **Column Numbers** |   **Column Labels**  |                                  **Description**                                  |
|:------------------:|:--------------------:|:---------------------------------------------------------------------------------:|
|          1         |       `initial`      | (string) subject's initials |
|          2         |         `age`        |                          (numeric) integer values (years)                         |
|          3         |         `sex`        |                    (categorical): binary ('male' and 'female')                    |
|          4         |        `race`        |                                   (categorical)                                   |
|        5-22        | `adhd_q1`-`adhd_q18` |                            (categorical) ordinal values                           |
|         23         |     `adhd_total`     |            (numeric): summary feature derived from `adhd_q1`-`adhd_q18`           |
|        24-38       |   `md_q1a`-`md_q3`   |                            (categorical) ordinal values                           |
|         39         |      `md_total`      |              (numeric): summary feature derived from `md_q1a`-`md_q3`             |
|         40         |       `alcohol`      |                           (categorical): ordinal values                           |
|         41         |         `thc`        |                           (categorical): ordinal values                           |
|         42         |       `cocaine`      |                           (categorical): ordinal values                           |
|         43         |     `stimulants`     |                           (categorical): ordinal values                           |
|         44         | `sedative_hypnotics` |                           (categorical): ordinal values                           |
|         45         |       `opioids`      |                           (categorical): ordinal values                           |
|         46         |     `court_order`    |                           (categorical): binary (yes/no)                          |
|         47         |      `education`     |                         (numeric): interger values (years)                        |
|         48         |   `hx_of_violence`   |                           (categorical): binary (yes/no)                          |
|         49         | `disorderly_conduct` |                           (categorical): binary (yes/no)                          |
|         50         |       `suicide`      |                           (categorical): binary (yes/no)                          |
|         51         |        `abuse`       |                           (categorical): ordinal values                           |
|         52         |    `non_subst_dx`    |                           (categorical): ordinal values                           |
|         53         |      `subst_dx`      |                           (categorical): ordinal values                           |
|         54         |     `psych_meds`     |                           (categorical): ordinal values                           |

The columns `adhd_q1`-`adhd_q18` and `md_q1a`-`md_q3` are summarized by the derivative columns `adhd_total` and `md_total` respectively. The **adhd** features correspond to an ADHD self-report survey whereas the **md** features give responses to a mood disorder self-report survey. For the initial Exploratory Data Analysis, the individual questions responses will be dropped in place of visualizations and summary statistics on the derived columns, `adhd_total` and `md_total`. A detailed analysis of the individual survey questions will be taken up in the Principal Components Analysis section.


Removing `loan_id`: this feature was found to have as many unique values as there are rows in the dataframe and is a record identification label. Therefore, we will drop this feature from the data:

```{r}
# remove loan ID
# df <- df %>%
#   select(-loan_id)
```

```{r}
# summary of each field
initial_eda_df <- df %>%
  select( -c( 5:22, 24:38 ) )
describe( initial_eda_df )
```

The output from `describe()` shows that many features have missing values. Next, we visualize the extent of the missing values using the `naniar` library.



### Missing Values

Use `naniar`'s `miss_var_summary()` and `vis_miss()` functions to summarize and visualize the missing values in the features of the dataset:
```{r}
# return a summary table of the missing data in each column
miss_var_summary(df)
```


```{r}
# visualize the amount of missing data for each feature
vis_miss( df, cluster = TRUE )
```

The figure above shows a grouped view of the missing values in each feature column. Overall, 2% of the values are missing from the dataset. Several features have no missing values (`education`, `applicantincome`, and `coapplicantincome`). Many of the features have relatively few missing values. However, the `credit_history` features is missing 8.14% of the data.

Explore the missing data further by using the `gg_miss_upset()` function to show patterns correlated missing values.

```{r}
gg_miss_upset( df )
```

The figure above shows that the vast majority of rows only have a singleton missing value; this is represented by the 5 bars in the left of the plot with only one dot to indicate the missing feature. However, a small minority or rows have 2-3 missing elements; this is indicated by multiple dots under the 5 bars to the right side of the plot.  

Since there are relatively few rows with multiple missing values, it would not adversely affect the analysis to remove them. The rest of the missing values can be dealt with by imputation. 

```{r}
# create a vector holding the sum of NAs for each row
count_na <- apply( df, 1, function(x) sum(is.na(x)))
# keep only the rows with less than 2 missing values
df <- df[count_na < 2,]
dim( df )
```

For a simple first approximation, we will use the `simputation` package$^1$ to fill NA values for categorical and numeric features with 'hot-deck' imputation (i.e. a values pulled at random from complete cases in the dataset).
```{r}

# # single imputation analysis
# df <- bind_shadow( df ) %>%
#   data.frame() %>%
#   simputation::impute_rhd(., credit_history ~ 1 ) %>%
#   simputation::impute_rhd(., loan_amount_term ~ 1 ) %>%
#   simputation::impute_rhd(., loanamount ~ 1 ) %>%
#   simputation::impute_rhd(., self_employed ~ 1 ) %>%
#   simputation::impute_rhd(., gender ~ 1 ) %>%
#   simputation::impute_rhd(., dependents ~ 1 ) %>%  
#   tbl_df()  %>%
#   select( -c(13:24) )

```


Confirm that we have filled all `NA` values:
```{r}
# return a summary table of the missing data in each column
miss_var_summary(df)

```

### Distributions of Numeric Variables

Now that the missing values have been imputed across the dataframe, we can explore the relationships of the variables in more depth. To start we visualize the distributions of the numeric variables grouped by the outcome of the target variable (`loan_status`): 

```{r}
# numeric distributions
df %>%
  select_if(is.numeric) %>%
  bind_cols(select(df, suicide)) %>%
  gather(var, val, -suicide) %>%
  ggplot(aes(x = val, fill = suicide)) +
  geom_density(alpha = .3) +
  facet_wrap(~var, scales = 'free', ncol = 2) +
  theme_bw() + 
  labs(x = element_blank(),
       y = element_blank(),
       title = 'Distribution of Numeric Variables'
       )

```
The distributions do not suggest any obviously significant differences when grouped by the target variable for any of the numeric features. It does not appear to be likely that either of these 3 features are correlated to `loan_status`. This can be confirmed with ANOVA$^2$:

```{r}
# # ANOVA for applicantincome
# applicantincome.aov <- aov(applicantincome ~ loan_status, data = df)
# # Summary of the analysis
# summary(applicantincome.aov)
# ```
# ```{r}
# # ANOVA for coapplicantincome
# coapplicantincome.aov <- aov(coapplicantincome ~ loan_status, data = df)
# # Summary of the analysis
# summary(coapplicantincome.aov)
```

```{r}
# # ANOVA for applicantincome
# loanamount.aov <- aov(loanamount ~ loan_status, data = df)
# # Summary of the analysis
# summary(loanamount.aov)
```
The p-values for all three ANOVA tests are very high indicating that there is no significant relationship between the features variables and the target.


### Correlation of Numeric Variables

Here we can look for correlations between feature variables
```{r function, echo = F}

plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}

```

```{r corrplot, fig.height = 8}

df_numeric <- df %>%
  select_if(is.numeric)
  # select(applicantincome, coapplicantincome, loanamount )

plot_corr_matrix(df_numeric, -1)
```
We can see a strong positive correlation between the features `applicantincome` and `loanamount`. There is a weak positive correlation between `coapplicantincome` and `loanamount`. Interestingly there is a weak negative correlation between `applicantincome` and `coapplicantincome`; presumptively due to a high-earning family being able to sustain with a single income.


### Distributions of Categorical Variables

No we turn to the categorical features to see if there are any strong relationships between them and the target variable.  
The following code will visualize the proportions of each target variable level for each level of a given feature:
```{r}
yes_count <- sum(df$suicide == 'Yes')
no_count <- sum(df$suicide == 'No')
  
df %>%
  select(!is.numeric) %>%
  gather(var, value, -suicide) %>%
  group_by(var, value, suicide) %>%
  dplyr::summarize(count = n(),
            .groups = 'drop') %>%
  dplyr::mutate(prop = count / ifelse(suicide == 'Yes', yes_count, no_count)) %>%
  ggplot(aes(x = value, y = prop, fill = suicide)) +
  geom_col(position = 'dodge') +
  facet_wrap(~var, scales = 'free') +
  theme_bw() +
  labs(y = 'Frequency Proportion',
       x = element_blank(),
       title = 'Frequency Distributions For Non-Numeric Variables') +
  scale_y_continuous(labels = percent_format(accuracy = 1))

```

When interpreting the categorical bar plots, differences between `loan_status` for a given feature-level suggest that a relationship exists between a feature and the target variable. For example, we see a clear difference between the Y/N bars for `credit_history`, `married` and `property_area` whereas the is little difference for the levels of `gender` and no noticeable difference for `self_employed`.  

The existence of a significant relationship between the categorical features and the target variable can be evaluated with a Chi-square test$^3$.

```{r}
# # Chi-square test for credit_history
# test <- chisq.test(table(df$credit_history, df$loan_status))
# test
```
```{r}
# # Chi-square test for married
# test <- chisq.test(table(df$married, df$loan_status))
# test
```
```{r}
# # Chi-square test for property_area
# test <- chisq.test(table(df$property_area, df$loan_status))
# test
```

```{r}
# # Chi-square test for education
# test <- chisq.test(table(df$education, df$loan_status))
# test
```
```{r}
# # Chi-square test for loan_amount_term
# test <- chisq.test(table(df$loan_amount_term, df$loan_status))
# test
```
```{r}
# # Chi-square test for dependents
# test <- chisq.test(table(df$dependents, df$loan_status))
# test
```
```{r}
# # Chi-square test for gender
# test <- chisq.test(table(df$gender, df$loan_status))
# test
```

```{r}
# # Chi-square test for self employed
# test <- chisq.test(table(df$self_employed, df$loan_status))
# test
```


### Data Prep

```{r data prep}
# impute NA  daaaang!, sorry Leo, I didn't see that you impute the NAs here until too late
# preproc <- preProcess(df, 'bagImpute')
# df2 <- predict(preproc, df)
df2 <- df 
# %>%
  # select( married, property_area, credit_history, education, loan_amount_term, loan_status )

# train test split
set.seed(101)
trainIndex <- createDataPartition(df2$suicide,
                                  p = 0.75,
                                  list = F)

train <- df2[trainIndex,]
test <- df2[-trainIndex,]

# cross validation train control
ctrl <- trainControl(method = 'cv', number = 10)
```



## Clustering Methods

### Finding K

In order to find patients of clusters, we first need to determine the number of clusters. Below, we'll use two common methods, evaluating the sum of squares and silhouette width, in order to decide how many clusters we'll use.

```{r cluster data prep}

# exclude names
df3 <- df_raw %>%
  select(-initial)

# impute null
preproc <- preProcess(df3, 'bagImpute')
df4 <- predict(preproc, df3)

```

#### Total Within Sum of Square

```{r cluster wss}

fviz_nbclust(df4, kmeans, method = "wss")

```

Based on the sum of squares for each value of K, we can use 2 clusters as the 'elbow' point.

#### Average Siluotte Width

```{r cluster silhouette}

fviz_nbclust(df4, kmeans, method = "silhouette")

```

Based on the average silhouette width of clusters, we can proceed with K = 2.

In this case, both methods have a consensus on two clusters.

### Calculating Clusters

Once the clusters are calculated, let's check the size of each cluster to make sure we're looking at different groups.

```{r cluster calc}

# 25 random starts
cluster_k2 <- kmeans(df4, 2, nstart = 25)

# add cluster number to dataframe
df4$cluster_k2 <- cluster_k2$cluster

# check cluster sizes
table(df4$cluster_k2)

```

Using K = 2, we created an almost even split of observations. Cluster 1 has 88 observations and cluster 2 has 87.

#### ADHD Total vs Suicide Plot

```{r cluster viz adhd}

# adhd vs suicide
fviz_cluster(cluster_k2, data = df4, c('adhd_total', 'suicide'))

```

The clusters created show a clear delineation for respondents based on `adhd_total`. This suggests that the clusters are separated based on levels of ADHD.

#### Mood Disorder Total vs Suicide Plot

```{r cluster viz mood disorder}

# mood disorder vs suicide
fviz_cluster(cluster_k2, data = df4, c('md_total', 'suicide'))

```

The results of the k2 cluster show considerable overlap based on `md_total`. This means that the clusters aren't separated by mood disorder levels.

### Relative Cluster Differences

Let's examine relative distributions for each cluster and variable.

```{r cluster plot function}

# function to plot relative variable distributions for clusters
cluster_plot <- function(start_col, end_col, plot = 'density', legend_position = 'bottom') {

  temp_df <- df4[,start_col:end_col] %>%
    bind_cols(cluster = factor(df4$cluster_k2)) %>%
    gather(var, val, -cluster)

  if (plot == 'density') {
    p <- ggplot(temp_df, aes(x = val, fill = cluster)) +
    geom_density(alpha = 0.3)
  } else if (plot == 'histogram') {
    p <- ggplot(temp_df, aes(x = val, fill = cluster)) +
    geom_histogram(alpha = 0.3)
  }
  
  p +
    facet_wrap(~var, scales = 'free') +
    theme_bw() +
    labs(fill = 'Cluster',
         x = element_blank(),
         y = element_blank()
         ) +
    theme(axis.ticks.y = element_blank(),
          axis.text.y = element_blank(),
          axis.text.x = element_text(size = 8),
          legend.position = legend_position)

}

```

#### Demographics

```{r, fig.height=3}

cluster_plot(1,3)

```

- Cluster 1 show a bimodal distribution of age, with modes as 25 and 45 years old. Cluster 2 has fewer younger people with a higher single mode.
- Cluster 2 is more likely to be African American.
- Cluster 1 is more likely to be female and Cluster 2 is more likely to be male.

#### ADHD 

```{r cluster adhd, fig.height = 7}

cluster_plot(4,22)

```

With clear differences for each question as well as the total, cluster 1 reported higher adhd values relative to cluster 2.

#### Mood Disorders

```{r cluster mood disorders, fig.height = 7}

cluster_plot(23,38)

```

Some questions have similiar distributions, but generally overall as well as altogether, cluster 1 is more likely to report higher in mood disorders.

#### Substance Abuse

```{r cluster substance abuse}

cluster_plot(39,44)

```

- Alcohol: Both clusters show similar usage patterns for alcohol, with cluster 1 having a slightly higher chance of using, abusing, and being dependent. Cluster 2 has a higher chance of not drinking.
- Cocaine: Cluster 1 is twice as likely as cluster 2 to not use cocaine and cluster 2 has a relatively slightly higher chance of being dependent.
- Opioids: Cluster 2 has a higher chance of not using.
- Sedative Hypnotics: Cluster 2 has a slighly higher chance of not using.
- Stimulants: Cluster 2 mostly does not use.
- THC: Cluster 1 more likely to both not use and also to be dependent.

#### Substance Abuse Levels

```{r cluster substance abuse levels, fig.height=3}

cluster_plot(51,53)

```

Cluster 2 has a higher chance of not taking non-substance abuse related drugs and is more likely to take one substance related drug.

#### Social

```{r}

cluster_plot(45,50)

```

Cluster 2 is less likely to experience abuse and suicide, and more likely to have a history of violence and be charged with disorderly conduct


## Principal Component Analysis

In this section we will explore the ADHD data set further using Principal Component Analysis (PCA). PCA is a dimensionality reduction method that can be very useful for detecting patterns in complex, feature-rich data. At it's core, PCA is a fairly simple linear algebra manipulation, an eigendecomposition. Basically, PCA maps the data onto a new set components that consist of orthogonal axes (eigenvectors) and magnitudes (eigenvalues). Each successive component is added such that it describes as much remaining variance in the data as possible. From the resulting components we can select a subset that describe most of the variance and use this new, lower-dimensional feature space as a more convenient way to observe relationships in the data that were previously masked by the data's complexity.  

To approach this analysis, we derive 2 sets of features from the original data. Data features that correspond to 1) self-reported answers for an ADHD survey, 2) self-reported answers for a Mood Disorder survey.

### PCA: ADHD Self-Report Survey

[The Adult ADHD Self-Reporting Scale Symptoms Checklist](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf) is a survey consisting of 18 questions that all range on an ordinal categorical scale: 'Never', 'Rarely', 'Sometimes', 'Often', 'Very Often'

#### Preprocessing

To prepare the ADHD survey responses we performed the following:  

* selected the ADHD survery question responses and exclude the feature `adhd_total` which gives a summary score for the survey
* PCA works best with numeric data, so we will transform the ordinal categorical features to numeric values
* evaluate the presence of missing values in the data

```{r}
# preparing the ADHD self-report questionnaire data
adhd_df <- df %>%
  select( starts_with("adhd_") ) %>%
  select( -'adhd_total' )
# recode all questions from factor to numeric values
adhd_df <- adhd_df %>%
  mutate_at( 
    vars(matches("adhd_q") ), 
         funs( recode_factor( ., 'Never'=0,'Rarely'=1,'Sometimes'=2,'Often'=3,'Very Often'=4 ) ) )  %>%
  mutate_if( is.factor, as.numeric )
paste( 'number of rows:', dim( adhd_df )[1] )


# return a summary table of the missing data in each column
paste( 'Summary of missing responses in the ADHD Self-Reporting Survey' )
miss_var_summary( adhd_df )
```

We see that there is only 1 missing value, so it seems reasonable to just drop it
```{r}
# remove missing
adhd_dna <- adhd_df %>%
  drop_na()
paste( 'number of rows:', dim( adhd_dna )[1] )
```

#### PCA from scratch

We can perform PCA easily from scratch and use the intermediate steps to learn more about the relationship of the individual question features to the ADHD total score:  

* First: we need to scale the data. 
  - This we de-mean the data and normalize the range of all features. Perhaps, redundant because all questions for this survey are on the same scale.
  - use the `scale()` function
* Second: we find the covariance matrix
  - use the `cov()` function

```{r}
# scale the data to facilitate PCA
adhd_df_scaled <- scale( adhd_dna )
# create a covariance matrix
adhd_df_cov <- cov( adhd_df_scaled )
```

Visualizing the covariance matrix
```{r}
pheatmap(adhd_df_cov, display_numbers = T, color = colorRampPalette(c('white','red'))(100), cluster_rows = F, cluster_cols = F, fontsize_number = 15)
```

The values off the diagonal of the matrix describe the degree of covariance between the two features. We see that some pairs of questions are more closely related than others. For instance, questions #5 and #13 have the highest covariance:  

* `adhd_q5` - "How often do you fidget or squirm with your hands or feet when you have to sit down for a long time?"
* `adhd_q13` - "How often do you feel restless or fidgety"

Perhaps it is no surprise that subject who report the frequency they fidget when sitting down for a long time report a similar incidence of general fidgeting/squirming.  

Another closely covarying pair of questions are #16 and #18  

* `adhd_q16` - "When youâ€™re in a conversation, how often do you find yourself finishing the sentences of the people you are talking to, before they can finish them themselves?"
* `adhd_q18` - "How often do you interrupt others when they are busy?"

Again, these are two very similar questions.  
It is equally interesting to evaluate the questions with the lowest covariance, questions #3 and #16(see above).  

* `adhd_q3` - "How often do you have problems remembering appointments or obligations?"

Indeed, conversation skills and the ability to remember appointments are two (subjectively) different task, so it is reasonable to not see a strong relation between the two.  

Moving on, the covariance matrix can now be used for an eigendecomposition to find the principal components that describe the most variance in the data
```{r}
# eigendecomposition of the covariance matrix
eig <- eigen( adhd_df_cov )
explained_variance <- eig$values
components <- eig$vectors
explained_variance
```

Now, by storing the eigen values in a dataframe, we can explore the amount of variance explained by the PCA components. Furthermore, we find the ratio of explained variance by normalizing the cumulative variance of the components
```{r}
compnum <- 1:length( explained_variance )
exv_cumsum <- cumsum( explained_variance )/length( explained_variance )
pca_res <- data.frame( compnum, explained_variance, exv_cumsum )
pca_res
```

Visualizing the cumulative explained variance described by the principal components with a Skree Plot:
```{r}
ggplot( pca_res, aes( x = compnum, y = exv_cumsum ) ) +
  geom_line() +
  ylim( c(0,1.1) ) +
  scale_x_continuous(breaks = seq(0, length( explained_variance ), by = 1)) +
  geom_hline( yintercept = 0.95, linetype = 'dotted', col = 'red') +
  annotate("text", x = 2, y = 0.98, 
           label = expression( "95%" ~ sigma), vjust = -0.5) +
  theme_minimal() +
  xlab( 'Principal Component Number' ) +
  ylab( 'Cummulative Explained Variance' ) +
  ggtitle( 'Scree Plot: Explained Variance of Principal Components' )
```

The figure above plots the cumulative explained variance of the ADHD survey questions and total score as a result of the eigendecomposition. We see that the first principal component accounts for `r paste0( round( exv_cumsum[1]*100,1 ), '%' )` of the total variance. Adding successive components gradually increases the cumulative explained variance. It takes as many as 14 components to describe 95% of the data's variance. Therefore, dimensionality reduction by way of PCA only moderately decreases the components necessary to adequately explain the data (with a threshold of 95%).

### PCA using `prcomp()`

We can determine the principal components of the ADHD data more quickly using the `prcomp()` function from the `stats` library.
```{r}
# use the prcomp function
adhd_dna.pca <- prcomp( adhd_dna, center = TRUE, scale = TRUE )
# print the pca summary
summary( adhd_dna.pca )
```

It is interesting to compare the Cumulative Proportion from the `prcomp()` output with the feature `pca_res$exv_cumsum`; the results are the same for both the built-in PCA function as well as are PCA from scratch (a relief!).

The PCA object that results from `prcomp()` can be used to visualize the components using the `ggbiplot` library:  
```{r}
library( ggbiplot )
ggbiplot( adhd_dna.pca )
```

The biplot shown above renders the data in the feature space of principal components PC1 and PC2; these are the components that explain most of the variance in the data. The scatterplot points show the projection of the data onto PC1 & PC2. The red arrows represent the original feature space that the data was represented in. We can see that the original features have the largest projection onto the PC1 axis. Relatively few original features have a sizeable projection onto PC2 (e.g. `adhd_q16` & `adhd_q17`). Additionally, these vectors all have a positive component along PC1 showing that the result for each question has a positive relation to PC1. The size of the angle between a given pair of vectors is inversely related to the correlation of the pair. For example, we see that `adhd_q3` and `adhd_q16` have the largest angle between them; this corresponds to our earlier observation looking at the covariance matrix. 

### Mood Disorders self-report

The [Mood Disorder Questionnaire](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC314375/) is a self-reporting survey that can be used to identify subjects that may be bipolar.  

We can perform a similar PCA analysis for the Mood Disorder survey results. Taking similar preprocessing steps: 1) select mood disorder question features, evaluate missing values... 
```{r}
# prepare the mood disorder data and recode all questions from factor to numeric values
md_df <- df %>%
  select( starts_with("md_") ) %>%
  select( -'md_total' ) %>%
  mutate_if( is.factor, as.numeric )
```

Checking the survey responses for missing values...
```{r}
# return a summary table of the missing data in each column
miss_var_summary( md_df )
```

Wonderful, there are no missing values, so we can proceed with PCA...
```{r}
# use the prcomp function
md_df.pca <- prcomp( md_df, center = TRUE, scale = TRUE )
# print the pca summary
summary( md_df.pca )
```

```{r,fig.width = 14}
ggbiplot( md_df.pca ) 
```

A similar pattern emerges for the Mood Disorder survey responses. We see that all the projections again have a major component along PC1 and `md_total` aligns almost perfectly with PC1. Again, we can infer relationships between features from the biplot: `md_q1c` and `md_q3` have the widest angle separating them and are therefore the least correlated features.  

## Gradient Boosting

Gradient Boosting builds a prediction model from an ensemble of smaller 'weak learner' models. In this section we will implement the gradient boosting approach to fit a binary classification model to features from the ADHD data set to predict whether a patient attempts suicide.

### Data Preparation

Before fitting a Gradient Boost Model, we will collect a new subset of features for this approach. Instead of including all features for individual questions on either of the summaries, we will only be using the total scores (`adhd_total` and `md_total`) because the total scores are composite features build from the scores on the questions. Additionally, the categorical responses 
```{r}
ordinal_cats <- c( 'alcohol', 'thc', 'cocaine', 'stimulants', 'sedative_hypnotics', 'opioids', 'suicide',
                   'court_order', 'hx_of_violence', 'disorderly_conduct', 'non_subst_dx', 'subst_dx')
gboost_df <- df %>%
  select( -starts_with("md_q") ) %>%
  select( -starts_with("adhd_q")) %>%
  select( -c('initial') ) %>%
  dplyr::mutate( across( all_of( ordinal_cats ), as.numeric ) )
paste( 'number of rows:', dim( gboost_df )[1] )
glimpse( gboost_df )
```
Evaluate the presense of missing data
```{r}
miss_var_summary( gboost_df )
```

```{r}
gg_miss_upset( gboost_df )
```

We see that the feature `psych_meds` is missing many values, so we will drop this feature from further consideration. Additionally, we see that there are approximately two dozen rows that are missing values for multiple features. Therefore, instead of imputing the missing, we will simply drop these rows.

```{r}
gboost_df <- gboost_df %>%
  select( -psych_meds ) %>%
  drop_na()
paste( 'number of rows:', dim( gboost_df )[1] )
miss_var_summary( gboost_df )
```


We will begin by splitting the data into train and test sets
```{r}
indexes = createDataPartition(gboost_df$suicide, p = .90, list = F)
train = gboost_df[indexes, ]
test = gboost_df[-indexes, ]
```

```{r}
# Train model with preprocessing & repeated cv
model_gbm <- caret::train(suicide ~ .,
                          data = train,
                          method = "gbm",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 3, 
                                                  verboseIter = FALSE),
                          verbose = 0)
print(model_gbm)
```

```{r}
#caret::confusionMatrix(
#  data = predict(model_gbm, test),
#  reference = test$suicide
#  )
```


## Support Vector Machine



## References

1. [ADHD symptoms and suicide attempts in adults with mood disorders: An observational naturalistic study](https://www.sciencedirect.com/science/article/pii/S2666915321001505)
2. [The Adult ADHD Self-Reporting Scale Symptoms Checklist](https://add.org/wp-content/uploads/2015/03/adhd-questionnaire-ASRS111.pdf)
3. [Mood Disorder Questionnaire](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC314375/)