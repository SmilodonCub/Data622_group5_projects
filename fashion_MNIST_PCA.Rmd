---
title: "Fashion MNIST PCA"
subtitle: "Group Final Project"
author: "David Blumenstiel, Bonnie Cooper, Robert Welk, Leo Yi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: no
    theme: paper
    highlight: tango
    font-family: Consolas
    #code_folding: hide
  pdf_document:
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

mark {
  background-color: whitesmoke;
  color: black;
}

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.width = 10)

options(scipen = 9)
```
<br>

## About this section for the final project

* PCA on fashion MNIST for dimensionality reduction
* SVM or something with Trees Classification before/after PCA

About Fashion MNIST:  
* 70,000 grayscale images
* 10 distinct image categories
* each image is 28x28 pixels (785 features)

### Importing the Data

libraries used:
```{r import}
library(keras)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)
```

helper functions for visualizing images
```{r}
show_digit = function(arr784, col = gray(12:1 / 12), ...) {
  image(matrix(as.matrix(arr784[-785]), nrow = 28)[, 28:1], col = col, labels=FALSE, xaxt = "n", yaxt = "n", ...)
}

# load image files
load_image_file = function(filename) {
  ret = list()
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
  close(f)
  data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}

# load label files
load_label_file = function(filename) {
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
  close(f)
  y
}
```

loading the data
```{r}
# load images
train = load_image_file("/home/bonzilla/Documents/MSDS/fashion-mnist/data/fashion/train-images-idx3-ubyte")
test  = load_image_file("/home/bonzilla/Documents/MSDS/fashion-mnist/data/fashion/t10k-images-idx3-ubyte")

# load labels
train_y = as.factor(load_label_file("/home/bonzilla/Documents/MSDS/fashion-mnist/data/fashion/train-labels-idx1-ubyte"))
test_y  = as.factor(load_label_file("/home/bonzilla/Documents/MSDS/fashion-mnist/data/fashion/t10k-labels-idx1-ubyte"))
```


### Exploratory Visualizations

Visualize a single fashion MNIST image:
```{r fig1, fig.height = 4, fig.width = 4, fig.align = "center"}
# view test image
show_digit(train[20000, ])
title('Example Image')
```

Visualiza a sampling of Fashion MNIST images: 
```{r fig2, fig.height = 10, fig.width = 10, fig.align = "center"}
# We plot 16 cherry-picked images from the training set
num = 10
par(mfrow=c(num, num), mar=c(0, 0.2, 1, 0.2))
for (i in 1:(num*num)) {
  show_digit(train[i, ])
}
```


Visualize each pixel's mean value and standard deviation across all Fashion MNIST images in the training dataset. Additionally visualize which pixels
```{r fig3, fig.height = 4, fig.width = 8, fig.align = "center"}
par(mfrow = c(1,2))

numcols = dim(train)[2]
train.ave = data.frame(pixel = apply(train, 2, mean))
show_digit( train.ave )
title( 'Pixel Mean Value' )

train.sd = data.frame(pixel = apply(train, 2, sd))
show_digit( train.sd )
title( 'Pixel Standard Deviation' )

par(mfrow = c(1,1))
```

The figure above left visualizes the mean pixel values across the `train` dataset. The figure to the right shows the standard deviations for each pixel. For both plots, higher intensity values are rendered as dark whereas low values are light. For many of the pixels in the image, the mean values is intermediate (gray) whereas the standard deviation is relatively high (dark). These pixels make up most of the variance in the dataset. However, we can see that there are two regions of pixels towards the image center where the mean pixel value is high (dark) while the pixel standard deviation is low (light). For these regions, the pixels have consistently high pixel values.  Towards the periphery of the image there are pixels with both low mean values (light) and low standard deviation (light). These pixels have consistently low values.  

The pixels with either consistently low or high values are of low information content and do not contribute much to models for image classification. These low information pixels add redundant features to the dataset. We can use PCA as a tool to reduce the dimensionality of the data such that we represent a maximum of data variance with fewer features. Such a manipulation is favorable because it will reduce the data complexity and time necessary to train models.  

The following figure highlights which pixels have a mean value less than 5% of the data range:
```{r fig4, fig.height = 4, fig.width = 4, fig.align = "center"}
train.avescale <- train.ave %>%
  mutate( pixscale = pixel/255,
          lowval = pixscale < 0.05 )

show_digit(train.avescale$lowval)
```


## PCA

```{r}
# use the prcomp function
train.pca <- prcomp( train, center = TRUE, scale = TRUE )
# print the pca summary
sumTrain <- summary( train.pca )
```

```{r}
#calculate total variance explained by each principal component
# train.pca$var_explained <- train.pca$sdev^2 / sum(train.pca$sdev^2)
# train.pca$cumvarexp <- cumsum( var_explained )
# train.pca$compnum <- 1:dim(train)[2]
df <- data.frame( t( sumTrain$importance ) )
df$compnum <- 1:dim(train)[2]
head( df )
```


Visualizing the cumulative explained variance described by the principal components with a Skree Plot:
```{r}
p1 <- ggplot( df, aes( x = compnum, y = Proportion.of.Variance ) ) +
  geom_line() +
  ylim( c(0,0.3) ) +
  xlim( c(0,20)) +
  geom_hline( yintercept = 0.01, linetype = 'dotted', col = 'red') +
  annotate("text", x = 2, y = 0.01, 
           label = expression( "1%" ~ sigma), vjust = -0.5) +
  theme_minimal() +
  xlab( 'Principal Component Number' ) +
  ylab( 'Proportion Explained Variance' ) +
  ggtitle( 'Skree plot' )

p2 <- ggplot( df, aes( x = compnum, y = Cumulative.Proportion ) ) +
  geom_line() +
  ylim( c(0,1.1) ) +
  xlim( c(0,400)) +
  geom_hline( yintercept = 0.95, linetype = 'dotted', col = 'red') +
  annotate("text", x = 2, y = 0.98, 
           label = expression( "95%" ~ sigma), vjust = -0.5) +
  theme_minimal() +
  xlab( 'Principal Component Number' ) +
  ylab( 'Cummulative Explained Variance' ) +
  ggtitle( 'Cumulative Explained Variance' )

grid.arrange( p1, p2, ncol=2)
```

From the skree plot, we can see a very sharp drop off in the proportion of explained variance. Principal Components greater than 13 account for less than 1% of the dataset's variance. However, the first 12 components only account for a cumulative variance of `r round( df$Cumulative.Proportion[12], 2)`


How many components account for 95% of the variance in the data?
```{r}
comp95 <- min(which(df$Cumulative.Proportion>=0.95))
comp95
```

Find truncated version of the data from the projections onto the first `r comp95` components
```{r}
trunc <- train.pca$x[,1:comp95] %*% t(train.pca$rotation[,1:comp95])

#and add the center (and re-scale) back to data
if(train.pca$scale != FALSE){
	trunc <- scale(trunc, center = FALSE , scale=1/train.pca$scale)
}
if(train.pca$center != FALSE){
    trunc <- scale(trunc, center = -1 * train.pca$center, scale=FALSE)
}
dim(trunc); dim(train)
```

Visualize the PCA smoothed images
```{r}
par(mfrow=c(4, 4), mar=c(0, 0.2, 1, 0.2))
for (i in 1:8) {
  nr <- i * 10
  show_digit(train[i, ])
}
for (i in 1:8) {
  nr <- i * 10
  show_digit(trunc[i, ])
}
```




## References

* [load the MNIST handwritten digits dataset into R as a tidy data frame](https://gist.github.com/daviddalpiaz/ae62ae5ccd0bada4b9acd6dbc9008706)