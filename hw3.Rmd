---
title: "Loan Approval Status"
subtitle: "Homework 3 Data 622 Section 2 Group 5"
author: "Bonnie Cooper, Orli Khaimova, Leo Yi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: no
    theme: paper
    highlight: tango
    font-family: Consolas
  pdf_document:
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

mark {
  background-color: whitesmoke;
  color: black;
}

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.width = 10)

options(scipen = 9)

```
<br>

## Introduction

We will be working with a dataset of loan approval status information. The task is to develop models to predict loan approval status with the given feature variables. After a preliminary exploratory data analysis, we will fit Linear Discriminant, K-Nearest Neighbors, Decision Trees and Random Forest models to a subset of the data and evaluate performance on a hold-out data set.

### Import Data

To begin, the following code will import the data and R necessary libraries:

```{r import}
library(tidyr)
library(dplyr)
library(ggplot2)
library(VIM)
library(corrplot)
library(purrr)
library(scales)
library(caret)
library(Hmisc)
library(naniar)
library(rattle)

# import data
url <- 'https://raw.githubusercontent.com/SmilodonCub/DATA622/master/Loan_approval.csv'
df <- read.csv(url, header=T, na.strings="")
```
<br>

## Exploratory Data Analysis

The following code will quantitatively and visually explore the nature of the loan approval dataset.  
We begin by describing the dataset features:
```{r}
# convert column names to lowercase
names(df) <- lapply(names(df), tolower)
names(df)
```

Use `dplyr`'s `glimpse()` function to take a quick look at the data structure. Followed by `Hmisc`'s `describe()` function to return some basic summary statistics about the dataframe features:
```{r}
# quick look at what the data structure looks like
glimpse(df)
```

```{r}
# summary of each field
describe(df)
```

From this output, we can summarize each dataset feature as follows:  

1. `loan_id` (ordinal): each entry is a unique value, therefore this feature is not informative for loan status
2. `gender` (categorical): 2 distinct values with missing data
3. `married` (categorical):  2 distinct values with missing data
4. `dependents` (categorical):  4 distinct values with missing data
5. `education` (categorical):  2 distinct values, no missing data
6. `self_employed` (categorical):  2 distinct values with missing data
7. `applicantincome` (numeric): value range, no missing data
8. `coapplicantincome` (numeric): value range, no missing data
9. `loanamount` (numeric): value range with missing data
10. `loan_amount_term` (numeric): relatively few unique values (10) with missing data 
11. `credit_history` (categorical): 2 distinct values with missing data
12. `property_area` (categorical): 3 distinct values, no missing data
13. `loan_status` (categorical): 2 distinct values, no missing data

Removing `loan_id`: this feature was found to have as many unique values as there are rows in the dataframe. `loan_id` is a record (row) identification label, therefore, we will drop this feature from the data:
```{r}
# remove loan ID
df <- df %>%
  select(-loan_id)
```


### Missing Values

The output from `describe()` reveals that many of the features have missing values. Here we use `naniar`'s `miss_var_summary()` and `vis_miss()` functions to summarize and visualize the missing values in the features of the dataset:
```{r}
# return a summary table of the missing data in each column
miss_var_summary(df)
```


```{r}
# visualize the amount of missing data for each feature
vis_miss( df, cluster = TRUE )
```

The figure above shows a grouped view of the missing values in each feature column. Overall, 2% of the values are missing from the dataset. Several features have no missing values (`education`, `applicantincome`, and `coapplicantincome`). Many of the features have relatively few missing values. However, the `credit_history` is missing 8.14% of the data; a substantial amount.

Explore the missing data further by using the `gg_miss_upset()` function to show patterns correlated missing values.
```{r}
gg_miss_upset( df )
```

The figure above shows that the vast majority of rows only have a singleton missing value; this is represented by the 5 bars in the left of the plot with only one dot to indicate the missing feature. However, a small minority or rows have 2-3 missing elements indicated by multiple, connected dots under the 5 bars to the right side of the plot.  

Since there are relatively few rows with multiple missing values, it would not adversely affect the power of the analysis to remove them. Imputation will be used to adress the remaining missing values. 

```{r}
# create a vector holding the sum of NAs for each row
count_na <- apply( df, 1, function(x) sum(is.na(x)))
# keep only the rows with less than 2 missing values
df <- df[count_na < 2,]
dim( df )
```

For a simple approximation, we will use the `simputation` package$^1$ to fill NA values for categorical and numeric features with 'hot-deck' imputation (i.e. a values pulled at random from complete cases in the dataset).
```{r}
# single imputation analysis
df <- bind_shadow( df ) %>%
  data.frame() %>%
  simputation::impute_rhd(., credit_history ~ 1 ) %>%
  simputation::impute_rhd(., loan_amount_term ~ 1 ) %>%
  simputation::impute_rhd(., loanamount ~ 1 ) %>%
  simputation::impute_rhd(., self_employed ~ 1 ) %>%
  simputation::impute_rhd(., gender ~ 1 ) %>%
  simputation::impute_rhd(., dependents ~ 1 ) %>%  
  tbl_df()  %>%
  select( -c(13:24) )
```


Confirm that we have filled all `NA` values:
```{r}
# return a summary table of the missing data in each column
miss_var_summary(df)
```

### Distributions of Numeric Variables

Now that the missing values have been imputed across the dataframe, we can explore the relationships of the variables in more depth. To start we visualize the distributions of the numeric variables grouped by the outcome of the target variable (`loan_status`): 

```{r}
# numeric distributions
df %>%
  select(applicantincome, coapplicantincome, loanamount ) %>%
  bind_cols(select(df, loan_status)) %>%
  gather(var, val, -loan_status) %>%
  ggplot(aes(x = val, fill = loan_status)) +
  geom_density(alpha = .3) +
  facet_wrap(~var, scales = 'free', ncol = 2) +
  theme_bw() + 
  labs(x = element_blank(),
       y = element_blank(),
       title = 'Distribution of Numeric Variables by Loan Approval Status'
       )

```
The distributions do not suggest any obviously significant differences when grouped by the target variable for any of the numeric features. It does not appear to be likely that either of these 3 features are correlated to `loan_status`. This can be confirmed with ANOVA$^2$:

```{r}
# ANOVA for applicantincome
applicantincome.aov <- aov(applicantincome ~ loan_status, data = df)
# Summary of the analysis
summary(applicantincome.aov)
```
```{r}
# ANOVA for coapplicantincome
coapplicantincome.aov <- aov(coapplicantincome ~ loan_status, data = df)
# Summary of the analysis
summary(coapplicantincome.aov)
```

```{r}
# ANOVA for applicantincome
loanamount.aov <- aov(loanamount ~ loan_status, data = df)
# Summary of the analysis
summary(loanamount.aov)
```
The p-values for all three ANOVA tests are very high indicating that there is no significant relationship between the features variables and the target.


### Correlation of Numeric Variables

Here we can look for correlations between feature variables
```{r function, echo = F}

plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}

```

```{r corrplot, fig.height = 8}

df_numeric <- df %>%
  select(applicantincome, coapplicantincome, loanamount )

plot_corr_matrix(df_numeric, -1)
```
We can see a strong positive correlation between the features `applicantincome` and `loanamount`. There is a weak positive correlation between `coapplicantincome` and `loanamount`. Interestingly there is a weak negative correlation between `applicantincome` and `coapplicantincome`; presumably due to a high-earning family being able to sustain with a single income.


### Distributions of Categorical Variables

No we turn to the categorical features to see if there are any strong relationships with the target variable.  
The following code will visualize the proportions of each target variable level for each level of a given feature:
```{r}
yes_count <- sum(df$loan_status == 'Y')
no_count <- sum(df$loan_status == 'N')
  
df %>%
  select(-applicantincome, -coapplicantincome, -loanamount ) %>%
  gather(var, value, -loan_status) %>%
  group_by(var, value, loan_status) %>%
  summarise(count = n(),
            .groups = 'drop') %>%
  mutate(prop = count / ifelse(loan_status == 'Y', yes_count, no_count)) %>%
  ggplot(aes(x = value, y = prop, fill = loan_status)) +
  geom_col(position = 'dodge') +
  facet_wrap(~var, scales = 'free') +
  theme_bw() +
  labs(y = 'Frequency Proportion',
       x = element_blank(),
       title = 'Frequency Distributions For Non-Numeric Variables') +
  scale_y_continuous(labels = percent_format(accuracy = 1))

```

When interpreting the categorical bar plots, differences between `loan_status` for a given feature-level suggest that a relationship exists between a feature and the target variable. For example, we see a clear difference between the Y/N bars for `credit_history`, `married` and `property_area`. However, there is little difference for the levels of `gender` and no noticeable difference for `self_employed`.  

The existence of a significant relationship between the categorical features and the target variable can be evaluated with a Chi-square test$^3$.

```{r}
cat_features <- c( 'self_employed', 'gender', 'dependents', 'loan_amount_term', 'education', 'property_area', 'married', 'credit_history' )
for(feature in cat_features){print( feature ); print( chisq.test(table(df[[feature]], df$loan_status)))}
```

From the results of the Chi-square test, only the following features have a statistically significant relation ($\alpha = 0.05$) to the target:  

* `credit_history`
* `married`
* `property_area`
* `education`

We will move forward using these four features to model `loan_status`.

### Data Prep

```{r data prep}
df2 <- df %>%
  select( married, property_area, credit_history, education, loan_status )

# train test split
set.seed(101)
trainIndex <- createDataPartition(df2$loan_status,
                                  p = 0.75,
                                  list = F)

train <- df2[trainIndex,]
test <- df2[-trainIndex,]

# cross validation train control
ctrl <- trainControl(method = 'cv', number = 10)
```


## LDA

**Model 1: Linear Discriminant Analysis** finds a linear combination of features to characterize the separation of target classes. We use the four key features variables that were selected during EDA to build our model (* `credit_history`, `married`, `property_area`, `education`).

```{r lda}

lda <- train(loan_status ~ .,
             data = train,
             method = 'lda',
             trControl = ctrl
             )

lda
```

## KNN

**Model 2: K-Nearest Neighbors** is a non-parametric method used here for classification

```{r knn}

knn <- train(loan_status ~ .,
             data = train,
             method = 'knn',
             trControl = ctrl,
             tuneLength = 10
             )

knn

```
To tune the hyperparameter 'k', the model tests multiple values (we specify a tune length of 10). The highest accuracy measure that resulted from cross-validation was $k=5$; this value is selected for our final KNN model run with the test data.


## Decision Tree

**Model 3: Decision Tree** is a non-parameteric model that uses simple, branched decision rules to optimise classification.

```{r decision trees}

cart <- train(loan_status ~ .,
              data = train,
              method = 'rpart',
              trControl = ctrl
              )

cart

```
```{r}
#summary( cart )
```

We can also visualize the resulting decision tree:

```{r}
fancyRpartPlot(cart$finalModel)
```

It is interesting that the decision tree does build much depth. Rather, the tree remains shallow using only `credit_history` for classification.


## Random Forest

**Model 4: Random Forest** is an ensemble method that combines multiple decision trees to optimize classification

```{r random forest}

rf <- train(loan_status ~ .,
            data = train,
            method = 'rf',
            trControl = ctrl
            )

rf

```


## Model Performance

### Confusion Matrix

Visualize example confusion matrix outcomes on the test data set for the four models:

```{r confusion matrix}
# calculate predictions
test$lda <- predict(lda, test)
test$knn <- predict(knn, test)
test$cart <- predict(cart, test)
test$rf <- predict(rf, test)

table(test$loan_status, test$lda, dnn = c('approval status','LDA predictions'))
table(test$loan_status, test$knn, dnn = c('approval status','KNN predictions'))
table(test$loan_status, test$cart, dnn = c('approval status','CART predictions'))
table(test$loan_status, test$rf, dnn = c('approval status','RF predictions'))
```

### Comparison of Model Accuracy

The confusion matrices show the results on the test data which can be used to calculate an accuracy score. However, to build a more robust estimate of each model's accuracy, we will collect metrics from multiple fits of the model. This is made very simple with a call to caret's `resamples()` function

```{r}
results <- resamples(list(LDA = lda,  DT = cart, kNN = knn, RF = rf))
summary( results )
```
From the output above, we will focus on the Accuracy measure to chose the best model of the four. We see that **Model 1: LDA** has the highest mean & median accuracy score.  

We can visualize this outcome with ggplot:  

```{r}
ggplot(results) + 
  labs(y = "Accuracy") + 
  theme_linedraw()
```

This graph visualizes that, although Model 1: LDA had the highest accuracy, Model 4: Random Field had very similar performance. 

## References

1. [Dealing with Missingness: Single Imputation](https://thomaselove.github.io/432-notes/dealing-with-missingness-single-imputation.html#single-imputation)  
2. [ANOVA test with `R`](http://www.sthda.com/english/wiki/one-way-anova-test-in-r)
3. [Chi-squared test of independence in `R`](https://statsandr.com/blog/chi-square-test-of-independence-in-r/)

<br><br>