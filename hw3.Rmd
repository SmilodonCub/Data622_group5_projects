---
title: "Loan Approval Status"
subtitle: "Homework 3 Data 622 Section 2 Group 5"
author: "Bonnie Cooper, Orli Khaimova, Leo Yi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: no
    theme: paper
    highlight: tango
    font-family: Consolas
  pdf_document:
    toc: yes
---

```{=html}
<style type="text/css">

code {
  font-family: "Consolas";
  font-size: 11px;
}

pre {
  font-family: "Consolas";
  font-size: 11px;
}

mark {
  background-color: whitesmoke;
  color: black;
}

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, fig.width = 10)

options(scipen = 9)

```
<br>

## Introduction

We will be working with a dataset of loan approval status information. The task is to develop models to predict loan approval status with the given feature variables. After a preliminary exploratory data analysis, we will fit Linear Discriminant, K-Nearest Neighbors, Decision Trees and Random Forest models to a subset of the data and evaluate performance on a hold-out data set.

### Import Data

To begin, the following code will import the data necessary libraries and import the dat:

```{r import}
library(tidyr)
library(dplyr)
library(ggplot2)
library(VIM)
library(corrplot)
library(purrr)
library(scales)
library(caret)
library(Hmisc)
library(naniar)

# import data
url <- 'https://raw.githubusercontent.com/SmilodonCub/DATA622/master/Loan_approval.csv'
df <- read.csv(url, header=T, na.strings="")
```
<br>

## Exploratory Data Analysis

The following code will quantitatively and visually explore the nature of the loan approval dataset.  
We begin by describing the dataset features:
```{r}
# convert column names to lowercase
names(df) <- lapply(names(df), tolower)
names(df)
```

Use `dplyr`'s `glimpse()` function to take a quick look at the data structure. Followed by `Hmisc`'s `describe()` function to return some basic summary statistics about the dataframe features:
```{r}
# quick look at what the data structure looks like
glimpse(df)
```

```{r}
# summary of each field
describe(df)
```

From this output, we can summarize each dataset feature as follows:  

1. `loan_id` (ordinal): each entry is a unique value, therefore this feature is not informative for loan status
2. `gender` (categorical): 2 distinct values with missing data
3. `married` (categorical):  2 distinct values with missing data
4. `dependents` (categorical):  4 distinct values with missing data
5. `education` (categorical):  2 distinct values, no missing data
6. `self_employed` (categorical):  2 distinct values with missing data
7. `applicantincome` (numeric): value range, no missing data
8. `coapplicantincome` (numeric): value range, no missing data
9. `loanamount` (numeric): value range with missing data
10. `loan_amount_term` (numeric): relatively few unique values (10) with missing data 
11. `credit_history` (categorical): 2 distinct values with missing data
12. `property_area` (categorical): 3 distinct values, no missing data
13. `loan_status` (categorical): 2 distinct values, no missing data

Removing `loan_id`: this feature was found to have as many unique values as there are rows in the dataframe and is a record identification label. Therefore, we will drop this feature from the data:
```{r}
# remove loan ID
df <- df %>%
  select(-loan_id)
```


### Missing Values

Use `naniar`'s `miss_var_summary()` and `vis_miss()` functions to summarize and visualize the missing values in the features of the dataset:
```{r}
# return a summary table of the missing data in each column
miss_var_summary(df)
```


```{r}
# visualize the amount of missing data for each feature
vis_miss( df, cluster = TRUE )
```

The figure above shows a grouped view of the missing values in each feature column. Overall, 2% of the values are missing from the dataset. Several features have no missing values (`education`, `applicantincome`, and `coapplicantincome`). Many of the features have relatively few missing values. However, the `credit_history` features is missing 8.14% of the data.

Explore the missing data further by using the `gg_miss_upset()` function to show patterns correlated missing values.
```{r}
gg_miss_upset( df )
```

The figure above shows that the vast majority of rows only have a singleton missing value; this is represented by the 5 bars in the left of the plot with only one dot to indicate the missing feature. However, a small minority or rows have 2-3 missing elements; this is indicated by multiple dots under the 5 bars to the right side of the plot.  

Since there are relatively few rows with multiple missing values, it would not adversely affect the analysis to remove them. The rest of the missing values can be dealt with by imputation. 

```{r}
# create a vector holding the sum of NAs for each row
count_na <- apply( df, 1, function(x) sum(is.na(x)))
# keep only the rows with less than 2 missing values
df <- df[count_na < 2,]
dim( df )
```

For a simple first approximation, we will use the `simputation` package to fill NA values for categorical and numeric features with 'hot-deck' imputation (i.e. a values pulled at random with replacement from complete values for the feature).
```{r}
df_median <- bind_shadow( df ) %>%
  impute_median_at() %>%
  add_label_shadow()
```


```{r}
# return a summary table of the missing data in each column
miss_var_summary(df_median)
```

### Correlation Plot

We can 

```{r function, echo = F}

plot_corr_matrix <- function(dataframe, significance_threshold){
  title <- paste0('Correlation Matrix for significance > ',
                  significance_threshold)
  
  df_cor <- dataframe %>% mutate_if(is.character, as.factor)
  
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)
  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > significance_threshold) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  # print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr,
           title=title,
           mar=c(0,0,1,0),
           method='color', 
           tl.col="black", 
           na.label= " ",
           addCoef.col = 'black',
           number.cex = .9)
}

```

```{r corrplot, fig.height = 8}

plot_corr_matrix(df, -1)

```

### Distributions of Numeric Variables

```{r}

# numeric distributions
df %>%
  select_if(is.numeric) %>%
  bind_cols(select(df, loan_status)) %>%
  gather(var, val, -loan_status) %>%
  ggplot(aes(x = val, fill = loan_status)) +
  geom_density(alpha = .3) +
  facet_wrap(~var, scales = 'free') +
  theme_bw() + 
  labs(x = element_blank(),
       y = element_blank(),
       title = 'Distribution of Numeric Variables by Loan Approval Status'
       )

```

Credit history seems to have a clear affect on loan approval status, with credit history meeting fully meeting guidelines showing higher rates of approvals.

### Distributions of Categorical Variables

```{r}

# df %>%
#   select_if(negate(is.numeric)) %>%
#   gather(var, value, -loan_status) %>%
#   ggplot(aes(x = value, fill = loan_status)) +
#   geom_bar(position = 'dodge') +
#   facet_wrap(~var, scales = 'free')

yes_count <- sum(df$loan_status == 'Y')
no_count <- sum(df$loan_status == 'N')
  
df %>%
  select_if(negate(is.numeric)) %>%
  gather(var, value, -loan_status) %>%
  group_by(var, value, loan_status) %>%
  summarise(count = n(),
            .groups = 'drop') %>%
  mutate(prop = count / ifelse(loan_status == 'Y', yes_count, no_count)) %>%
  ggplot(aes(x = value, y = prop, fill = loan_status)) +
  geom_col(position = 'dodge') +
  facet_wrap(~var, scales = 'free') +
  theme_bw() +
  labs(y = 'Frequency Proportion',
       x = element_blank(),
       title = 'Frequency Distributions For Non-Numeric Variables') +
  scale_y_continuous(labels = percent_format(accuracy = 1))

```

### Data Prep

```{r data prep}
# impute NA
preproc <- preProcess(df, 'bagImpute')
df2 <- predict(preproc, df)

# train test split
set.seed(101)
trainIndex <- createDataPartition(df2$loan_status,
                                  p = 0.75,
                                  list = F)

train <- df2[trainIndex,]
test <- df2[-trainIndex,]

# cross validation train control
ctrl <- trainControl(method = 'cv', number = 10)
```


## LDA

```{r lda}

lda <- train(loan_status ~ .,
             data = train,
             method = 'lda',
             trControl = ctrl
             )

lda
```

## KNN

```{r knn}

knn <- train(loan_status ~ .,
             data = train,
             method = 'knn',
             trControl = ctrl
             )

knn

```

## Decision Tree

```{r decision trees}

cart <- train(loan_status ~ .,
              data = train,
              method = 'rpart',
              trControl = ctrl
              )

cart

```

## Random Forest

```{r random forest}

rf <- train(loan_status ~ .,
            data = train,
            method = 'rf',
            trControl = ctrl
            )

rf

```

## Model Performance

### Confusion Matrix

```{r confusion matrix}
# calculate jpredictions
test$lda <- predict(lda, test)
test$knn <- predict(knn, test)
test$cart <- predict(cart, test)
test$rf <- predict(rf, test)

table(test$loan_status, test$lda, dnn = c('approval status','LDA predictions'))
table(test$loan_status, test$knn, dnn = c('approval status','KNN predictions'))
table(test$loan_status, test$cart, dnn = c('approval status','CART predictions'))
table(test$loan_status, test$rf, dnn = c('approval status','RF predictions'))
```

### Comparison of Model Accuracy

```{r}
# model accuracy table
data.frame(accuracy = rbind(
  sum(test$loan_status == test$lda) / nrow(test),
  sum(test$loan_status == test$knn) / nrow(test),
  sum(test$loan_status == test$cart) / nrow(test),
  sum(test$loan_status == test$rf) / nrow(test)
),
  row.names = c('LDA',
                'KNN',
                'CART', 
                'Random Forest'
                )
)
```

## References


<br><br>